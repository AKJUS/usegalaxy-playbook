<?xml version="1.0"?>
<!--
    This file is maintained by Ansible - CHANGES WILL BE OVERWRITTEN
-->
<job_conf>
    <plugins workers="3">
        <plugin id="dynamic" type="runner">
            <!-- These live in the virtualenv -->
            <param id="rules_module">usegalaxy.jobs.rules</param>
        </plugin>
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner">
            <param id="drmaa_library_path">/usr/lib64/libdrmaa.so</param>
            <param id="invalidjobexception_retries">5</param>
            <param id="internalexception_retries">5</param>
        </plugin>
        <expand macro="pulsar_plugin" id="jetstream_iu" />
        <expand macro="pulsar_plugin" id="jetstream_iu_nagios" />
        <expand macro="pulsar_plugin" id="jetstream_tacc" />
        <expand macro="pulsar_plugin" id="jetstream_tacc_nagios" />
        <expand macro="pulsar_plugin" id="stampede" />
        <expand macro="pulsar_plugin" id="stampede_nagios" />
        <expand macro="pulsar_plugin" id="bridges" />
        <expand macro="pulsar_plugin" id="bridges_nagios" />
        <plugin id="pulsar_jetstream_autoscaling_test" type="runner" load="galaxy.jobs.runners.pulsar:PulsarMQJobRunner">
            <param id="amqp_url">{{ galaxy_job_conf_amqp_url }}</param>
            <param id="galaxy_url">https://{{ galaxy_instance_hostname }}</param>
            <param id="manager">jetstream_autoscaling_test</param>
        </plugin>
    </plugins>
    <handlers default="handlers" assign_with="db-skip-locked" max_grab="8">
        {#
        <handler id="test_handler0" tags="handlers">
            <plugin id="local"/>
            <plugin id="slurm"/>
            <plugin id="pulsar_jetstream_autoscaling_test"/>
        </handler>
        #}
        <expand macro="normal_handler" id="0" />
        <expand macro="normal_handler" id="1" />
        <expand macro="multi_handler" id="2" />
        <expand macro="multi_handler" id="3" />
        <handler id="galaxy_datamanager">
            <plugin id="slurm"/>
        </handler>
    </handlers>
    <destinations default="dynamic_normal_reserved">
        <!-- dynamic function for setting roundup multi walltime, also handles the explicit stampede resource selector -->
        <expand macro="dynamic_destination" dest="local_stampede_select_dynamic_walltime" />
        <!-- dynamic function for the multi-or-bridges selector -->
        <expand macro="dynamic_destination" dest="multi_bridges_select" />
        <!-- dynamic method for the stampede-only selector -->
        <expand macro="dynamic_destination" dest="stampede_select" />
        <!-- dynamic method for the bridges-only selector -->
        <expand macro="dynamic_destination" dest="bridges_select" />
        <!-- dynamic memory rule for the naive variant caller -->
        <expand macro="dynamic_destination" dest="nvc_dynamic_memory" />

        <expand macro="dynamic_destination" dest="rnastar" />

        <!-- dynamic local/reserved destinations-->
        <!-- test resubmit
        <expand macro="slurm_reserved_destination" dest="normal" partition="normal" native_specification="- -cpus-per-task=1 - -time=36:00:00" />
        -->
        <destination id="dynamic_normal_reserved" runner="dynamic">
            <param id="type">python</param>
            <param id="function">dynamic_normal_reserved</param>
        </destination>
        <destination id="slurm_normal" runner="slurm">
            <param id="nativeSpecification">--partition=normal,jsnormal --nodes=1 --cpus-per-task=1 --time=36:00:00</param>
            <expand macro="dest_local_env" java_mem="7g"/>
            <resubmit condition="memory_limit_reached" destination="dynamic_normal_16gb_reserved" />
        </destination>
        <destination id="reserved_normal" runner="slurm">
            <param id="nativeSpecification">--clusters=roundup --partition=reserved,jsreserved --nodes=1 --cpus-per-task=1 --time=36:00:00</param>
            <expand macro="dest_local_env" java_mem="7g" />
        </destination>
        <!-- creates destinations: dynamic_@DEST@_reserved, slurm_@DEST@, reserved_@DEST@ -->
        <expand macro="slurm_reserved_destination" dest="normal_16gb" partition="normal" native_specification="--cpus-per-task=1 --time=36:00:00 --mem-per-cpu=15360" java_mem="15g" />
        <expand macro="slurm_reserved_destination" dest="normal_32gb" partition="normal" native_specification="--cpus-per-task=1 --time=36:00:00 --mem-per-cpu=30720" java_mem="30g" />
        <expand macro="slurm_reserved_destination" dest="normal_64gb" partition="normal" native_specification="--cpus-per-task=1 --time=36:00:00 --mem-per-cpu=61440" java_mem="60g" />
        <!-- CPUs/tasks and time are set by dynamic rule -->
        <!--
        <expand macro="slurm_reserved_destination" dest="multi" partition="multi,jsmulti" native_specification="- -cpus-per-task=6 - -time=36:00:00" java_mem="28g" />
        -->
        <destination id="dynamic_multi_reserved" runner="dynamic">
            <param id="type">python</param>
            <param id="function">dynamic_multi_reserved</param>
        </destination>
        <destination id="slurm_multi" runner="slurm">
            <param id="nativeSpecification">--clusters=roundup --partition=multi,jsmulti --nodes=1</param>
            <expand macro="dest_local_env" java_mem="28g"/>
        </destination>
        <destination id="reserved_multi" runner="slurm">
            <param id="nativeSpecification">--clusters=roundup --partition=reserved,jsreserved --nodes=1 --cpus-per-task=6 --time=36:00:00</param>
            <expand macro="dest_local_env" java_mem="28g" />
        </destination>

        <!-- singularity test destinations -->
        <destination id="slurm_singularity" runner="slurm">
            <param id="outputs_to_working_directory">true</param>
            <param id="singularity_enabled">true</param>
            <param id="singularity_volumes">$galaxy_root:ro,$tool_directory:ro,$job_directory:rw,$working_directory:rw,/galaxy-repl/test/scratch:rw,/galaxy-repl/test/files:ro,/galaxy-repl/data/:ro</param>
            <!-- It'd be nice if we could fall back to a bas container and use TS/Conda deps, but these options don't work like that ("require_container" isn't working at all for some reason)
                <param id="singularity_default_container_id">/cvmfs/sandbox.galaxyproject.org/singularity/mulled/centos:7.4.1708</param>
                <param id="require_container">false</param>
             -->
        </destination>

        <!-- local destinations -->
        <expand macro="slurm_destination" dest="multi_development" partition="normal" native_specification="--cpus-per-task=2 --time=00:30:00 --mem-per-cpu=5120" java_mem="10g" />
        <expand macro="slurm_destination" dest="multi_16c_128gb" partition="multi" native_specification="--cpus-per-task=16 --time=24:00:00 --mem-per-cpu=7680" java_mem="120g" />

        <!-- dynamic modified destinations: do not use directly -->
        <expand macro="slurm_destination" dest="multi_dynamic_walltime" partition="multi" native_specification="--cpus-per-task=6" />
        <!-- <resubmit condition="walltime_reached" destination="stampede_normal" handler="multi"/> -->
        <expand macro="slurm_destination" dest="normal_dynamic_mem" partition="normal" native_specification="--cpus-per-task=1 --time=24:00:00" />
        <!-- the "team dev" destination -->
        <expand macro="slurm_destination" dest="dynamic" partition="reserved" native_specification="--time=144:00:00 --mem-per-cpu=5120" />

        <!-- jetstream destinations -->
        <destination id="jetstream_multi" runner="dynamic">
            <!-- this destination exists for the dynamic runner to read the native spec for testing -->
            <param id="nativeSpecification">--time=36:00:00 --nodes=1 --partition=multi</param>
        </destination>
        <expand macro="jetstream_destination" id="multi" site="iu" native_specification="--partition=multi --nodes=1 --time=36:00:00" />
        <expand macro="jetstream_nagios_destination" site="iu" />
        <expand macro="jetstream_destination" id="multi" site="tacc" native_specification="--partition=multi --nodes=1 --time=36:00:00" />
        <expand macro="jetstream_nagios_destination" site="tacc" />
        <expand macro="jetstream_destination" id="xlarge" site="tacc" native_specification="--partition=xlarge --nodes=1 --time=36:00:00" />

        <!-- test -->
        <expand macro="jetstream_destination" id="normal" site="iu" native_specification="--partition=normal --nodes=1 --time=36:00:00 --cpus-per-task=3 --mem=7942" />
        <expand macro="jetstream_destination" id="normal_16gb" site="iu" native_specification="--partition=normal --nodes=1 --time=36:00:00 --exclusive" />
        <expand macro="jetstream_destination" id="reserved" site="iu" native_specification="--partition=reserved --nodes=1 --time=36:00:00" />

        <!-- stampede destinations -->
        <!-- memory_mb vals are node spec mem minus 4 GB -->
        <expand macro="stampede_destination" id="normal" native_specification="--partition=normal --nodes=1 --account=TG-MCB140147" memory_mb="94208" />
        <expand macro="stampede_destination" id="development" native_specification="--partition=development --nodes=1 --account=TG-MCB140147" memory_mb="94208" />
        <expand macro="stampede_destination" id="skx_normal" native_specification="--partition=skx-normal --nodes=1 --account=TG-MCB140147" memory_mb="192512" />
        <expand macro="stampede_destination" id="skx_development" native_specification="--partition=skx-dev --nodes=1 --account=TG-MCB140147" memory_mb="192512" />
        <expand macro="stampede_destination" id="long" native_specification="--partition=long --nodes=1 --account=TG-MCB140147" memory_mb="94208" />
        <expand macro="stampede_nagios_destination" />

        <!-- New generic multi destinations -->
        <expand macro="dynamic_destination" dest="multi" />
        <!-- ntasks and time are set by the dynamic rule -->
        <expand macro="slurm_destination" dest="mpi_multi" partition="multi" native_specification="" java_mem="4g" />
        <expand macro="stampede_mpi_destination" id="normal" native_specification="--account=TG-MCB140147 --partition=normal --nodes=1" />
        <expand macro="stampede_mpi_destination" id="skx_normal" native_specification="--account=TG-MCB140147 --partition=skx-normal --nodes=1" />
        <expand macro="slurm_destination" dest="mpi_development" partition="normal" native_specification="" java_mem="4g" />
        <expand macro="stampede_mpi_destination" id="development" native_specification="--account=TG-MCB140147 --partition=development --nodes=1" />
        <expand macro="stampede_mpi_destination" id="skx_development" native_specification="--account=TG-MCB140147 --partition=skx-dev --nodes=1" />

        <!-- bridges destinations -->
        <!-- walltime and mem are set dynamically -->
        <!-- use constraint to avoid running on xl nodes, which do not mount /pylon5 -->
        <expand macro="bridges_destination" id="normal" native_specification="-p LM --constraint=LM&amp;EGRESS" />
        <!-- 147456 MB == 144 GB (3 cores) (128GB is the minimum for LM) -->
        <expand macro="bridges_destination" id="development" native_specification="-p LM --constraint=LM&amp;EGRESS -t 00:30:00 --mem=147456" />
        <expand macro="bridges_nagios_destination" />

        <!-- local destination for nagios handler checks -->
        <destination id="local" runner="local"/>

        {#
        <!-- jetstream autoscaling test destination -->
        <destination id="jetstream_autoscaling_test" runner="pulsar_jetstream_autoscaling_test">
            <!--
            <param id="remote_metadata">true</param>
            -->
            <param id="submit_native_specification">--cpus-per-task=1 --mem-per-cpu=1792 --time=24:00:00</param>
            <expand macro="dest_pulsar_common_params" />
            <param id="jobs_directory">/export/test/jobs</param>
            <param id="remote_property_galaxy_home">/export/test/galaxy</param>
            <!-- this doesn't work, set in supervisor environment instead
            <param id="remote_property_galaxy_virtual_env">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv</param>
            -->
            <!--
            <param id="file_action_config">{{ galaxy_config_dir }}/pulsar_jetstream_actions.yml</param>
            <env id="PATH">/jetstream/scratch0/test/conda/envs/set_metadata@20171114/bin:$PATH</env>
            -->
            <expand macro="dest_tmp_env" />
        </destination>
        #}
    </destinations>
    <tools>
        <!-- explicit local multi jobs -->
        <expand macro="multi_tool" id="gemini_load" />
        <expand macro="multi_tool" id="dada2_assignTaxonomyAddspecies" />
        <expand macro="multi_tool" id="deeptools_bam_compare" />
        <expand macro="multi_tool" id="deeptools_bam_coverage" />
        <expand macro="multi_tool" id="deeptools_bam_pe_fragmentsize" />
        <expand macro="multi_tool" id="deeptools_bigwig_compare" />
        <expand macro="multi_tool" id="deeptools_compute_gc_bias" />
        <expand macro="multi_tool" id="deeptools_compute_matrix" />
        <expand macro="multi_tool" id="deeptools_correct_gc_bias" />
        <expand macro="multi_tool" id="deeptools_multi_bam_summary" />
        <expand macro="multi_tool" id="deeptools_multi_bigwig_summary" />
        <expand macro="multi_tool" id="deeptools_plot_coverage" />
        <expand macro="multi_tool" id="deeptools_plot_fingerprint" />
        <expand macro="multi_tool" id="kraken" />
        <expand macro="multi_tool" id="ideas" />
        <!-- resubmit to stampede jobs -->
        <expand macro="stampede_resubmit_tool" id="bwa" />
        <expand macro="stampede_resubmit_tool" id="bwa_mem" />
        <expand macro="stampede_resubmit_tool" id="bwa_wrapper" />
        <expand macro="stampede_resubmit_tool" id="bowtie2" />
        <expand macro="stampede_resubmit_tool" id="bowtie_wrapper" />
        <expand macro="stampede_resubmit_tool" id="tophat" />
        <expand macro="stampede_resubmit_tool" id="tophat2" />
        <expand macro="stampede_resubmit_tool" id="cuffdiff" />
        <expand macro="stampede_resubmit_tool" id="cufflinks" />
        <expand macro="stampede_resubmit_tool" id="cuffmerge" />
        <expand macro="stampede_resubmit_tool" id="cuffnorm" />
        <expand macro="stampede_resubmit_tool" id="cuffquant" />
        <expand macro="stampede_resubmit_tool" id="stringtie" />
        <expand macro="stampede_resubmit_tool" id="hisat2" />
        <expand macro="stampede_resubmit_tool" id="prokka" />
        <expand macro="stampede_resubmit_tool" id="rbc_mafft" />
        <expand macro="stampede_resubmit_tool" id="kc_align" />
        <tool id="hyphy_absrel" destination="dynamic_mpi" handler="multi" resources="mpi" />
        <tool id="hyphy_fel" destination="dynamic_mpi" handler="multi" resources="mpi" />
        <tool id="hyphy_gard" destination="dynamic_mpi" handler="multi" resources="mpi" />
        <tool id="hyphy_meme" destination="dynamic_mpi" handler="multi" resources="mpi" />
        <!-- explicit stampede jobs -->
        <expand macro="stampede_tool" id="megablast_wrapper" />
        <!-- new LASTZ wrapper is not multicore, but it may need more memory
             <expand macro="stampede_tool" id="lastz_wrapper_2" /> -->
        <expand macro="stampede_tool" id="bwa_color_wrapper" />
        <expand macro="stampede_tool" id="bowtie_color_wrapper" />
        <!-- bridges jobs -->
        <expand macro="bridges_tool" id="trinity_psc" />
        <expand macro="bridges_tool" id="trinity" />
        <expand macro="bridges_tool" id="spades" />
        <expand macro="bridges_tool" id="rnaspades" />
        <expand macro="bridges_tool" id="unicycler" />
        <expand macro="bridges_tool" id="star_fusion" />
        <tool id="rna_star" destination="dynamic_rnastar" handler="multi"/>
        <!-- trackster jobs -->
        <tool id="cufflinks" destination="slurm_multi_trackster">
            <param id="source">trackster</param>
        </tool>
        <!-- dynamic memory tools -->
        <tool id="naive_variant_caller" destination="dynamic_nvc_dynamic_memory"/>
        <!-- local/jetstream tools -->
        <tool id="align_families" destination="dynamic_local_stampede_select_dynamic_walltime" handler="multi" resources="local_or_stampede"/>
        <!-- roundup multi/jetstream/bridges tools -->
        <tool id="rna_star" destination="dynamic_multi_bridges_select" handler="multi"  resources="multi_or_bridges"/>
        <tool id="data_manager_star_index_builder" destination="dynamic_multi_bridges_select" handler="multi"  resources="multi_or_bridges"/>
        <tool id="align_families" destination="slurm_multi" handler="multi" resources="local_or_stampede"/>
        <!-- 16GB tools -->
        <!--
        <tool id="sailfish" destination="dynamic_normal_16gb_reserved"/>
        <tool id="bamtools" destination="dynamic_normal_16gb_reserved"/>
        <tool id="varscan" destination="dynamic_normal_16gb_reserved"/>
        <tool id="scatterplot_rpy" destination="dynamic_normal_16gb_reserved"/>
        <tool id="correct_barcodes" destination="dynamic_normal_16gb_reserved"/>
        <tool id="rseqc_read_duplication" destination="dynamic_normal_16gb_reserved"/>
        <tool id="rseqc_RPKM_saturation" destination="dynamic_normal_16gb_reserved"/>
        <tool id="rseqc_bam2wig" destination="dynamic_normal_16gb_reserved"/>
        <tool id="seqtk_sample" destination="dynamic_normal_16gb_reserved"/>
        -->
        <!-- 64GB tools -->
        <tool id="wig_to_bigWig" destination="dynamic_normal_64gb_reserved"/>
        <tool id="CONVERTER_bedgraph_to_bigwig" destination="dynamic_normal_64gb_reserved"/>
        <!-- nagios checks -->
        <!-- run wherever it lands
        <tool id="echo_test_cluster"/>
        -->
        <tool id="echo_test_handler0" destination="local" handler="test_handler0"/>
        <tool id="echo_test_handler1" destination="local" handler="test_handler1"/>
        <tool id="echo_test_jetstream_iu" destination="jetstream_iu_nagios" handler="multi"/>
        <tool id="echo_test_jetstream_tacc" destination="jetstream_tacc_nagios" handler="multi"/>
        <tool id="echo_test_stampede" destination="stampede_nagios" handler="multi"/>
        <tool id="echo_test_bridges" destination="bridges_nagios" handler="multi"/>
        <!-- data managers -->
        <tool id="data_manager_fetch_genome_all_fasta" destination="reserved_normal"/>
        <tool id="bwa_mem_index_builder_data_manager" destination="reserved_normal_64gb"/>
        <tool id="data_manager_bowtie2_index_builder" destination="reserved_normal"/>
        <tool id="data_manager_bowtie_index_builder" destination="reserved_normal"/>
        <tool id="data_manager_hisat2_index_builder" destination="reserved_normal_16gb"/>
        <tool id="kraken_database_builder" destination="slurm_multi_16c_128gb" handler="multi"/>
        <tool id="rna_star_index_builder_data_manager" destination="slurm_multi_16c_128gb" handler="multi"/>
        <!-- singularity test tools -->
        <tool id="tp_cat" destination="slurm_singularity" handler="test_handler0" />
        <tool id="jq" destination="slurm_singularity" handler="test_handler0" />
        {# FIXME
        <!-- jetstream autoscaling test tools -->
        <tool id="tp_find_and_replace" destination="jetstream_autoscaling_test" handler="test_handler0" />
        #}
    </tools>
    <resources default="default">
        <group id="default"></group>
        <group id="local_or_stampede">multi_compute_resource,cores,time</group>
        <group id="multi_or_bridges">multi_bridges_compute_resource</group>
        <group id="stampede">stampede_compute_resource</group>
        <group id="bridges">bridges_compute_resource</group>
        <group id="mpi">multi_compute_resource,cores,time</group>
    </resources>
    <limits>
        <!--
        <limit type="registered_user_concurrent_jobs">6</limit>
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <limit type="job_walltime">49:00:00</limit>
        <limit type="output_size">50G</limit>
        <!-- per-destination per-user limits -->
        <limit type="destination_user_concurrent_jobs" id="slurm_normal">4</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_normal_16gb">1</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_normal_64gb">1</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_multi">2</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_multi_dynamic_walltime">2</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_multi_development">1</limit>
        <limit type="destination_user_concurrent_jobs" id="slurm_multi_trackster">1</limit>
        <limit type="destination_user_concurrent_jobs" id="stampede_normal">4</limit>
        <limit type="destination_user_concurrent_jobs" id="stampede_skx_normal">4</limit>
        <limit type="destination_user_concurrent_jobs" id="stampede_development">1</limit>
        <limit type="destination_user_concurrent_jobs" id="stampede_skx_development">1</limit>
        <!--
        <limit type="destination_user_concurrent_jobs" id="bridges_normal">6</limit>
        <limit type="destination_user_concurrent_jobs" id="bridges_development">1</limit>
        -->
        <limit type="destination_user_concurrent_jobs" id="jetstream">4</limit>
        <limit type="destination_user_concurrent_jobs" id="jetstream_tacc_xlarge">1</limit>
        <!-- per-destination total limits -->
        <limit type="destination_total_concurrent_jobs" id="stampede_normal">50</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_skx_normal">50</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_mpi_normal">20</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_mpi_skx_normal">20</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_development">1</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_skx_development">1</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_mpi_development">1</limit>
        <limit type="destination_total_concurrent_jobs" id="stampede_mpi_skx_development">1</limit>
        <!--
        <limit type="destination_total_concurrent_jobs" id="bridges_normal">10</limit>
        -->
        <limit type="destination_total_concurrent_jobs" id="bridges_development">4</limit>
    </limits>
    <macros>
        <xml name="pulsar_plugin" tokens="id">
            <plugin id="pulsar_@ID@" type="runner" load="galaxy.jobs.runners.pulsar:PulsarMQJobRunner">
                <param id="amqp_url">{{ galaxy_job_conf_amqp_url }}</param>
                <param id="galaxy_url">https://{{ galaxy_instance_hostname }}</param>
                <param id="manager">@ID@</param>
                <param id="persistence_directory">/srv/galaxy/{{ galaxy_instance_codename }}/var/pulsar_amqp_ack</param>
                <param id="amqp_acknowledge">True</param>
                <param id="amqp_ack_republish_time">1200</param>
                <param id="amqp_consumer_timeout">2.0</param>
                <param id="amqp_publish_retry">True</param>
                <param id="amqp_publish_retry_max_retries">60</param>
            </plugin>
        </xml>
        <xml name="normal_handler" tokens="id">
            <handler id="test_handler@ID@" tags="handlers">
                <plugin id="local"/>
                <plugin id="slurm"/>
            </handler>
        </xml>
        <xml name="multi_handler" tokens="id">
            <handler id="test_handler@ID@" tags="multi">
                <plugin id="slurm"/>
                <plugin id="pulsar_stampede"/>
                <plugin id="pulsar_stampede_nagios"/>
                <plugin id="pulsar_bridges"/>
                <plugin id="pulsar_bridges_nagios"/>
                <plugin id="pulsar_jetstream_iu"/>
                <plugin id="pulsar_jetstream_iu_nagios"/>
                <plugin id="pulsar_jetstream_tacc"/>
                <plugin id="pulsar_jetstream_tacc_nagios"/>
            </handler>
        </xml>
        <xml name="dynamic_destination" tokens="dest">
            <destination id="dynamic_@DEST@" runner="dynamic">
                <param id="type">python</param>
                <param id="function">dynamic_@DEST@</param>
            </destination>
        </xml>
        <xml name="slurm_destination" tokens="dest,partition,native_specification,java_mem" token_java_mem="7g">
            <destination id="slurm_@DEST@" runner="slurm">
                <param id="nativeSpecification">--partition=@PARTITION@ --nodes=1 @NATIVE_SPECIFICATION@</param>
                <expand macro="dest_local_env" java_mem="@JAVA_MEM@ "/>
            </destination>
        </xml>
        <xml name="slurm_reserved_destination" tokens="dest,partition,native_specification,java_mem" token_java_mem="7g">
            <expand macro="dynamic_destination" dest="@DEST@_reserved" />
            <expand macro="slurm_destination" dest="@DEST@" partition="@PARTITION@" native_specification="@NATIVE_SPECIFICATION@" java_mem="@JAVA_MEM@" />
            <destination id="reserved_@DEST@" runner="slurm">
                <param id="nativeSpecification">--clusters=roundup --partition=reserved --nodes=1 @NATIVE_SPECIFICATION@</param>
                <expand macro="dest_local_env" java_mem="@JAVA_MEM@" />
            </destination>
        </xml>
        <!-- some unfortunate duplication here because macro expansions in the macro attribute of an expand tag is not supported -->
        <xml name="jetstream_destination" tokens="id,site,native_specification">
            <!-- the "jetstream" tag is for limits -->
            <destination id="jetstream_@SITE@_@ID@" runner="pulsar_jetstream_@SITE@" tags="jetstream">
                <param id="remote_metadata">true</param>
                <param id="submit_native_specification">@NATIVE_SPECIFICATION@</param>
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_jetstream_params" />
            </destination>
        </xml>
        <xml name="jetstream_nagios_destination" tokens="site">
            <destination id="jetstream_@SITE@_nagios" runner="pulsar_jetstream_@SITE@_nagios">
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_jetstream_params" />
            </destination>
        </xml>
        {# TODO: Now that we use --ntasks for regular (non-MPI) jobs, these destinations can be merged, but it will
                 require changes to the mpi dynamic rule that I don't have time to make at the moment #}
        <xml name="stampede_destination" tokens="id,native_specification,memory_mb">
            <destination id="stampede_@ID@" runner="pulsar_stampede">
                <param id="remote_metadata">true</param>
                <param id="submit_native_specification">@NATIVE_SPECIFICATION@</param>
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_stampede_params" />
                <!-- Stampede assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -->
                <env exec="GALAXY_SLOTS=$SLURM_NTASKS" />
                <!-- Mem=0 if the Slurm -mem param is not used, which is not allowed on Stampede2 -->
                <env exec="GALAXY_MEMORY_MB=@MEMORY_MB@" />
            </destination>
        </xml>
        <xml name="stampede_mpi_destination" tokens="id,native_specification">
            <destination id="stampede_mpi_@ID@" runner="pulsar_stampede">
                <param id="remote_metadata">true</param>
                <param id="submit_native_specification">@NATIVE_SPECIFICATION@</param>
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_stampede_params" />
                <!-- Stampede assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -->
                <env exec="GALAXY_SLOTS=$SLURM_NTASKS" />
            </destination>
        </xml>
        <xml name="stampede_nagios_destination">
            <destination id="stampede_nagios" runner="pulsar_stampede_nagios">
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_stampede_params" />
            </destination>
        </xml>
        <xml name="bridges_destination" tokens="id,native_specification">
            <destination id="bridges_@ID@" runner="pulsar_bridges">
                <param id="remote_metadata">true</param>
                <param id="submit_native_specification">@NATIVE_SPECIFICATION@</param>
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_bridges_params" />
            </destination>
        </xml>
        <xml name="bridges_nagios_destination">
            <destination id="bridges_nagios" runner="pulsar_bridges_nagios">
                <expand macro="dest_pulsar_common_params" />
                <expand macro="dest_pulsar_bridges_params" />
            </destination>
        </xml>
        <xml name="dest_local_env" tokens="java_mem" token_java_mem="7g">
            <!-- cloudmap tools are still using R 2.11(!) from here, also the genome diversity tools use things in /galaxy/software -->
            <env id="PATH">/galaxy/{{ galaxy_instance_codename }}/linux-x86_64/bin:/galaxy/software/linux-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin</env>
            <expand macro="dest_tmp_env" />
            <env id="_JAVA_OPTIONS">$_JAVA_OPTIONS -Xmx@JAVA_MEM@ -Xms256m</env>
        </xml>
        <xml name="dest_tmp_env">
            <env id="XDG_DATA_HOME">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/xdg/data</env>
            <env id="TEMP">$(dirname ${BASH_SOURCE[0]})/_job_tmp</env>
            <env id="TMPDIR">$TEMP</env>
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=$TEMP</env>
            <env exec="mkdir -p $TEMP" />
        </xml>
        <xml name="dest_pulsar_common_params">
            <param id="transport">curl</param>
            <param id="default_file_action">remote_transfer</param>
            <param id="dependency_resolution">remote</param>
            <param id="rewrite_parameters">true</param>
            <param id="submit_user_email">$__user_email__</param>
            <env id="LC_ALL">C</env>
        </xml>
        <xml name="dest_pulsar_jetstream_params">
            <param id="jobs_directory">/jetstream/scratch0/{{ galaxy_instance_codename }}/jobs</param>
            <!-- this doesn't work, set in supervisor environment instead
            <param id="remote_property_galaxy_virtual_env">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv</param>
            -->
            <param id="remote_property_galaxy_home">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy</param>
            <param id="file_action_config">{{ galaxy_config_dir }}/pulsar_jetstream_actions.yml</param>
            <env id="PATH">/jetstream/scratch0/test/conda/envs/set_metadata@20171114/bin:$PATH</env>
            <expand macro="dest_tmp_env" />
        </xml>
        <xml name="dest_pulsar_stampede_params">
            <param id="jobs_directory">/scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/</param>
            <!-- this doesn't work
            <param id="remote_property_galaxy_virtual_env">/work/galaxy/{{ galaxy_instance_codename }}/galaxy/venv</param>
            -->
            <!-- this used to work but doesn't now either, set in supervisor environment instead, however, the Pulsar
                 client still requires it to be set when remote_metadata is enabled -->
            <param id="remote_property_galaxy_home">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy</param>
            <param id="file_action_config">{{ galaxy_config_dir }}/pulsar_stampede_actions.yml</param>
            <env exec="eval `/opt/apps/lmod/lmod/libexec/lmod bash purge`" />
            <env exec="ulimit -c 0" />
        </xml>
        <xml name="dest_pulsar_bridges_params">
            <param id="jobs_directory">/pylon5/mc48nsp/xcgalaxy/{{ galaxy_instance_codename }}/staging/</param>
            <!-- see stampede comments
            <param id="remote_property_galaxy_virtual_env">/pylon5/mc48nsp/xcgalaxy/{{ galaxy_instance_codename }}/galaxy/venv</param>
            -->
            <param id="remote_property_galaxy_home">/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy</param>
            <param id="file_action_config">{{ galaxy_config_dir }}/pulsar_bridges_actions.yml</param>
            <env exec="eval `modulecmd sh purge`" />
            <!-- https://bugs.openjdk.java.net/browse/JDK-7085890 -->
            <env id="PATH">/pylon5/mc48nsp/xcgalaxy/openjdk8/bin:$PATH</env>
            <env id="_JAVA_OPTIONS">-Dsun.zip.disableMemoryMapping=true</env>
            <!-- SPAdes as run by Unicycler is leaving cores in Pulsar's site-packages dir -->
            <env exec="ulimit -c 0" />
        </xml>
        <xml name="multi_tool" tokens="id">
            <tool id="@ID@" destination="dynamic_multi_reserved" handler="multi" />
        </xml>
        <xml name="stampede_tool" tokens="id">
            <tool id="@ID@" destination="dynamic_stampede_select" handler="multi" resources="stampede"/>
        </xml>
        <xml name="stampede_resubmit_tool" tokens="id">
            <tool id="@ID@" destination="dynamic_multi_reserved" handler="multi" resources="local_or_stampede" />
        </xml>
        <xml name="bridges_tool" tokens="id">
            <tool id="@ID@" destination="dynamic_bridges_select" handler="multi" resources="bridges"/>
        </xml>
    </macros>
</job_conf>
