---
# TODO: tools_conf, common vars should be merged into here

# This is in the all group_vars so that you can override values in galaxyservers group_vars if needed

galaxy_job_conf_environments:

  # these envs have id 'slurm_{{ id }}'
  slurm_environments:

    # non-singularity environments
    # TODO: these are for transition only and should not be used once all tools are transitioned to Singularity
    - id: normal_py2_direct
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      use_singularity: false
      extra_params: {use_metadata_binary: true}
      tags: [normal_limit]
      append_envs:
        - file: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin/activate
        - name: GALAXY_VIRTUAL_ENV
          value: None
        - name: PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/deps/_py2/bin:$PATH
    - id: test_normal_py2_direct
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      use_singularity: false
      extra_params: {use_metadata_binary: true}
      tags: [test_limit]
      append_envs:
        - file: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin/activate
        - name: GALAXY_VIRTUAL_ENV
          value: None
        - name: PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/deps/_py2/bin:$PATH
    - id: normal_direct
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      use_singularity: false
      java_mem: 7
      tags: [normal_limit]
    - id: test_normal_direct
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      use_singularity: false
      java_mem: 3
      tags: [test_limit]
    - id: normal_16gb_direct
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00 --mem=15360
      use_singularity: false
      java_mem: 7
      tags: [normal_limit]
    - id: normal_32gb_direct
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=24:00:00 --mem=30720
      use_singularity: false
      java_mem: 7
      tags: [normal_limit]
    - id: normal_64gb_direct
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=4:00:00 --mem=61440
      use_singularity: false
      java_mem: 7
      tags: [normal_limit]
    - id: multi_direct
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      use_singularity: false
      java_mem: 7
      tags: [multi_direct, multi_limit]
    - id: multi_py2_direct
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      use_singularity: false
      extra_params: {use_metadata_binary: true}
      tags: [multi_py2_direct, multi_limit]
      append_envs:
        - file: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin/activate
        - name: GALAXY_VIRTUAL_ENV
          value: None
        - name: PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/deps/_py2/bin:$PATH
    - id: multi_long_direct
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=72:00:00
      use_singularity: false
      java_mem: 28
      tags: [multi_long, multi_long_limit]

    # training environments
    - id: training_direct
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      use_singularity: false
      java_mem: 3
      tags: [training_limit]
    - id: training
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_long
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=02:00:00 --mem=3840
      java_mem: 3
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_large
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:30:00 --mem=15360
      java_mem: 15
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_xlarge
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:20:00 --mem=23040
      java_mem: 22
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_multi
      native_specification: --partition=reserved --nodes=1 --ntasks=2 --time=01:20:00 --mem=7680
      java_mem: 7
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_multi_large
      native_specification: --partition=reserved --nodes=1 --ntasks=4 --time=01:20:00 --mem=15360
      java_mem: 15
      extra_params: {singularity_no_mount: null}
      tags: [training_limit]
    - id: training_resolv_fix
      runner: slurm
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      tags: [training_limit]
      extra_params: {singularity_run_extra_arguments: --no-mount=tmp}
      volumes: "{{ galaxy_job_conf_singularity_volumes.local + ['/etc/pki:ro', '/etc/ssl:ro'] }}"

    # legacy/conda environments
    - id: normal_conda
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      tags: [normal_limit]
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          # TODO: commit to CVMFS
          # image is https://github.com/natefoo/usegalaxy.org-legacy-environment
          identifier: /corral4/main/singularity/usegalaxy.org-legacy-environment--0
    - id: test_normal_conda
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      tags: [test_limit]
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: /corral4/main/singularity/usegalaxy.org-legacy-environment--0
    - id: multi_conda
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      tags: [multi_limit]
      container_override:
        - type: singularity
          shell: /bin/sh
          resolve_dependencies: true
          identifier: /corral4/main/singularity/usegalaxy.org-legacy-environment--0
    - id: normal_py2
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:2.7.16
      tags: [normal_limit]
    - id: test_normal_py2
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:2.7.16
      tags: [test_limit]
    - id: multi_py2
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:2.7.16
      tags: [multi_py2, multi_limit]
    - id: normal_galaxy_env
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
      append_envs:
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
    - id: test_normal_galaxy_env
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      tags: [test_limit]
      extra_params: {singularity_no_mount: null}
      append_envs:
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
    - id: normal_resolv_fix
      runner: slurm
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
      # https://github.com/bioconda/bioconda-recipes/issues/11583
      volumes: "{{ galaxy_job_conf_singularity_volumes.local + ['/etc/pki:ro', '/etc/ssl:ro'] }}"
    - id: test_normal_resolv_fix
      runner: slurm
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      tags: [test_limit]
      extra_params: {singularity_no_mount: null}
      # https://github.com/bioconda/bioconda-recipes/issues/11583
      volumes: "{{ galaxy_job_conf_singularity_volumes.local + ['/etc/pki:ro', '/etc/ssl:ro'] }}"

    # upload should not need a ton of resources but also needs RW access to the FTP dir
    #- id: upload
    #  native_specification: --partition=normal --nodes=1 --ntasks=1 --time=12:00:00 --mem=3840
    #  java_mem: 3
    #  volumes: "{{ galaxy_job_conf_singularity_volumes + [galaxy_nginx_upload_dir ~ ':rw'] }}"
    #  create_test_env: true
    #  tags:
    #    - normal

    # normal environments
    - id: normal
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00
      java_mem: 7
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
    - id: test_normal
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=00:50:00 --mem=3840
      java_mem: 3
      tags: [test_limit]
      extra_params: {singularity_no_mount: null}
    - id: normal_16gb
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=36:00:00 --mem=15360
      java_mem: 15
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
    - id: normal_32gb
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=24:00:00 --mem=30720
      java_mem: 30
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
    - id: normal_64gb
      native_specification: --partition=normal --nodes=1 --ntasks=1 --time=4:00:00 --mem=61440
      java_mem: 60
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}

    # multicore environments
    - id: multi
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      java_mem: 28
      tags: [multi, multi_limit]
      extra_params: {singularity_no_mount: null}
    - id: multi_development
      native_specification: --partition=normal --nodes=1 --ntasks=2 --time=00:30:00
      java_mem: 15
    - id: multi_long
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=72:00:00
      java_mem: 28
      tags: [multi_long, multi_long_limit]
      extra_params: {singularity_no_mount: null}
    - id: multi_resolv_fix
      runner: slurm
      native_specification: --partition=multi --nodes=1 --ntasks=6 --time=36:00:00
      tags: [multi_limit]
      extra_params: {singularity_no_mount: null}
      # https://github.com/bioconda/bioconda-recipes/issues/11583
      volumes: "{{ galaxy_job_conf_singularity_volumes.local + ['/etc/pki:ro', '/etc/ssl:ro'] }}"

    # priority (reserved) environments
    - id: reserved_normal
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=36:00:00
      java_mem: 7
      tags: [reserved]
      extra_params: {singularity_no_mount: null}
    - id: reserved_normal_direct
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=36:00:00
      use_singularity: false
      java_mem: 7
      tags: [reserved]
      extra_params: {singularity_no_mount: null}
    - id: reserved_normal_16gb
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=36:00:00 --mem=15360
      java_mem: 15
      tags: [reserved]
      extra_params: {singularity_no_mount: null}
    - id: reserved_normal_32gb
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=24:00:00 --mem=30720
      java_mem: 30
      tags: [reserved]
      extra_params: {singularity_no_mount: null}
    - id: reserved_normal_64gb
      native_specification: --partition=reserved --nodes=1 --ntasks=1 --time=4:00:00 --mem=61440
      java_mem: 60
      tags: [reserved]
      extra_params: {singularity_no_mount: null}
    - id: reserved_multi
      native_specification: --partition=reserved --nodes=1 --ntasks=6 --time=36:00:00 --mem=30720
      java_mem: 28
      tags: [reserved]
      extra_params: {singularity_no_mount: null}

  jetstream2_environments:

    # non-singularity environments
    - id: normal_direct
      native_specification: --partition=normal --nodes=1 --time=36:00:00
      use_singularity: false
      java_mem: 5
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
    - id: multi_direct
      native_specification: --partition=multi --nodes=1 --time=36:00:00
      use_singularity: false
      java_mem: 28
      tags: [multi_direct, multi_limit]
      extra_params: {singularity_no_mount: null}
    - id: large_direct
      native_specification: --partition=large --nodes=1 --time=36:00:00
      use_singularity: false
      java_mem: 58
      tags: [multi_largemem_direct, multi_largemem_limit]
      extra_params: {singularity_no_mount: null}

    # test GxITs through Pulsar/Slurm
    #
    # NOTE: to be usable in production, you would probably need to:
    #   1. keep at least 1 node active since JS2 boot times are currently ~2.5 minutes.
    #   2. operate a docker image cache or pass-thru registry since pull time for the jupyter image is currently ~4 minutes.
    #
    # FIXME: disabling --user with docker_set_user below is not ideal and causes outputs to be written as root, but
    # without it you have permissions problems e.g. from Jupyter:
    #
    #Traceback (most recent call last):
    #  File "/opt/conda/bin/jupyter-trust", line 10, in <module>
    #    sys.exit(TrustNotebookApp.launch_instance())
    #  File "/opt/conda/lib/python3.8/site-packages/jupyter_core/application.py", line 254, in launch_instance
    #    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
    #  File "/opt/conda/lib/python3.8/site-packages/traitlets/config/application.py", line 844, in launch_instance
    #    app.initialize(argv)
    #  File "/opt/conda/lib/python3.8/site-packages/traitlets/config/application.py", line 87, in inner
    #    return method(app, *args, **kwargs)
    #  File "/opt/conda/lib/python3.8/site-packages/jupyter_core/application.py", line 229, in initialize
    #    self.migrate_config()
    #  File "/opt/conda/lib/python3.8/site-packages/jupyter_core/application.py", line 155, in migrate_config
    #    migrate()
    #  File "/opt/conda/lib/python3.8/site-packages/jupyter_core/migrate.py", line 245, in migrate
    #    with open(os.path.join(env['jupyter_config'], 'migrated'), 'w') as f:
    #PermissionError: [Errno 13] Permission denied: '/home/jovyan/.jupyter/migrated'
    - id: gxit
      native_specification: --partition=gxit --nodes=1 --ntasks=1 --mem=2968 --time=12:00:00
      use_singularity: false
      java_mem: 2
      tags: ['gxit_limit']
      remote_metadata: false
      # will default to remote if not singularity, so test:
      #dependency_resolution: local
      extra_params:
        docker_enabled: true
        docker_set_user: null
        container_resolvers:
          - type: explicit
        require_container: true
        container_monitor_command: /opt/galaxy-job-execution/bin/galaxy-container-monitor
        container_monitor_result: callback
        container_monitor_get_ip_method: command:/usr/bin/tailscale ip -4

    # singularity environments
    - id: normal
      native_specification: --partition=normal --nodes=1 --time=36:00:00
      java_mem: 5
      tags: [normal_limit]
      extra_params: {singularity_no_mount: null}
    - id: multi
      native_specification: --partition=multi --nodes=1 --time=36:00:00
      java_mem: 28
      tags: [multi, multi_limit]
      extra_params: {singularity_no_mount: null}
    - id: large
      native_specification: --partition=large --nodes=1 --time=36:00:00
      java_mem: 58
      tags: [multi_largemem, multi_largemem_limit]
      extra_params: {singularity_no_mount: null}
    - id: xl
      native_specification: --partition=xl --nodes=1 --time=36:00:00
      java_mem: 120
      tags: [multi_xlargemem, multi_xlargemem_limit]
      extra_params: {singularity_no_mount: null}
    - id: training_multi_large
      native_specification: --partition=tpv --nodes=1 --ntasks=4 --mem=15360 --time=02:00:00
      java_mem: 14
      tags: [training_limit]
      extra_params: {singularity_no_mount: null}
    - id: training_multi_xlarge
      native_specification: --partition=tpv --nodes=1 --ntasks=8 --mem=30720 --time=02:00:00
      java_mem: 28
      tags: [training_limit]
      extra_params: {singularity_no_mount: null}
    - id: training_large
      native_specification: --partition=tpv --nodes=1 --ntasks=1 --mem=15360 --time=00:40:00
      java_mem: 14
      tags: [training_limit]
      extra_params: {singularity_no_mount: null}
    - id: training_large_long
      native_specification: --partition=tpv --nodes=1 --ntasks=1 --mem=15360 --time=02:00:00
      java_mem: 14
      tags: [training_limit]
      extra_params: {singularity_no_mount: null}

    # wrap tar to work around ceph issue causing "tar: foo: file changed as we read it"
    - id: xl_wraptar
      native_specification: --partition=xl --nodes=1 --time=36:00:00
      java_mem: 120
      tags: [multi_xlargemem_limit]
      volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream_wraptar }}"
      extra_params: {singularity_no_mount: null}

  stampede_environments:
    - id: normal
      memory_mb: 94208
      java_mem: 88
      native_specification: --partition=normal --nodes=1 --account=TG-MCB140147 --ntasks=68 --time=36:00:00
      tags: [stampede_limit]
    - id: development
      memory_mb: 94208
      java_mem: 88
      native_specification: --partition=development --nodes=1 --account=TG-MCB140147 --ntasks=68 --time=00:30:00
      tags: [stampede_limit]
    - id: skx_normal
      memory_mb: 192512
      java_mem: 184
      native_specification: --partition=skx-normal --nodes=1 --account=TG-MCB140147 --ntasks=48 --time=36:00:00
      tags: [stampede_limit]
    - id: skx_development
      memory_mb: 192512
      java_mem: 184
      native_specification: --partition=skx-dev --nodes=1 --account=TG-MCB140147 --ntasks=48 --time=00:30:00
      tags: [stampede_limit]
    - id: long
      memory_mb: 94208
      java_mem: 88
      native_specification: --partition=long --nodes=1 --account=TG-MCB140147 --ntasks=68 --time=60:00:00
      tags: [multi_long, multi_long_limit, stampede_limit]

  frontera_environments:
    - id: small
      native_specification: --partition=small --nodes=1 --ntasks=56 --time=48:00:00 --account=ASC21038
      memory_mb: 194560
    - id: development
      native_specification: --partition=development --nodes=1 --ntasks=56 --time=00:30:00 --account=ASC21038
      memory_mb: 194560

  bridges_environments:
    - id: normal
      native_specification: --partition=RM --time=24:00:00 --nodes=1 --ntasks=64
    - id: shared_128gb
      native_specification: --partition=RM-shared --time=24:00:00 --nodes=1 --ntasks=64
    - id: shared_64gb
      native_specification: --partition=RM-shared --time=24:00:00 --nodes=1 --ntasks=32
    - id: extreme_1tb
      native_specification: --partition=EM --time=24:00:00 --nodes=1 --ntasks=24
    - id: development
      native_specification: --partition=RM-shared --time=00:30:00 --nodes=1 --ntasks=8
