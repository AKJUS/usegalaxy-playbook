---

destinations:

  _roundup_common:
    abstract: true
    runner: slurm
    max_accepted_cores: 2
    max_accepted_mem: 8
    context:
      partition: normal
      time: 24:00:00
    params:
      native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition={partition}"
      tmp_dir: true
      outputs_to_working_directory: true
      metadata_strategy: extended
    env:
    - execute: ulimit -c 0
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C

  roundup_conda_direct:
    inherits: _roundup_common
    scheduling:
      accept:
      - roundup
      - cvmfs
      require:
      - conda

  roundup_conda_singularity:
    inherits: _roundup_common
    params:
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      require_container: true
      container:
        - type: singularity
          shell: "/bin/bash"
          resolve_dependencies: true
          identifier: "/corral4/main/singularity/usegalaxy.org-legacy-environment--0"
      accept:
      - roundup
      - cvmfs
      require:
      - conda
      - singularity

  roundup:
    inherits: _roundup_common
    params:
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    scheduling:
      accept:
      - roundup
      - cvmfs
      require:
      - singularity

  jetstream2:
    runner: jetstream2
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 128 if js2_partition == 'vgp' else 32
    max_mem: 980 if js2_partition == 'vgp' else 120
    context:
      time: 48:00:00
      # js2_partition is set on role and (default) tool entitites
      #partition: tpv
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition={js2_partition}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer_tus
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs-vgp
    env:
    - execute: ulimit -c 0
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITY_CACHEDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_cache
    - name: SINGULARITY_TMPDIR
      value: /jetstream2/scratch/{{ galaxy_instance_codename }}/singularity_tmp
    scheduling:
      accept:
      - jetstream2
      - cvmfs
      # for TPV shared DB
      - docker
      prefer:
      - jetstream2
      require:
      - pulsar
      - singularity

  jetstream2_resize_shm:
    inherits: jetstream2
    params:
      submit_native_specification: "--nodes=1 --time={time} --partition=resize-shm"
    scheduling:
      require:
      - resize-shm
      - jetstream2
      - pulsar
      #reject:
      #- offline

  jetstream2_gpu:
    inherits: jetstream2
    max_accepted_cores: 32
    max_accepted_gpus: 1.0
    max_accepted_mem: 125
    max_cores: 32
    max_gpus: 1.0
    max_mem: 125
    params:
      submit_native_specification: "--nodes=1 --ntasks={int(cores)} --mem={round(mem*1024)} --time={time} --partition={js2_gpu_partition}"
      singularity_run_extra_arguments: --nv -B /etc/OpenCL/vendors:/usr/local/etc/OpenCL/vendors
    env:
    # gmx_sim uses this, maybe others?
    - name: SINGULARITYENV_GPU_AVAILABLE
      value: "1"
    - execute: nvidia-modprobe -u -c=0
    scheduling:
      require:
      - gpu
      - pulsar
      - singularity

  bridges2:
    runner: bridges2
    min_accepted_cores: 32
    min_accepted_mem: 64
    max_accepted_cores: 128
    max_accepted_mem: 512
    max_mem: 500
    context:
      time: 47:00:00
    params:
      tmp_dir: true
      outputs_to_working_directory: false
      metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 120
      params:
        submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem/2)*2000}"
      env:
      # TODO: this should be set properly for RM-shared but you should test
      - name: GALAXY_SLOTS
        value: "{int(mem/2)}"
      - name: GALAXY_MEMORY_MB
        value: "{int(mem*1024)}"
    - if: mem > 120 and mem <= 250
      params:
        submit_native_specification: "--partition=RM --time={time} --nodes=1 --ntasks={int(cores)}"
      env:
      - name: GALAXY_SLOTS
        value: "128"
      - name: GALAXY_MEMORY_MB
        value: "{int(250*1024)}"
    - if: mem > 250
      params:
        submit_native_specification: "--partition=RM-512 --time={time} --nodes=1"
      env:
      - name: GALAXY_SLOTS
        value: "128"
      - name: GALAXY_MEMORY_MB
        value: "{int(500*1024)}"
    scheduling:
      accept:
      - bridges2
      - hpc
      - cvmfs
      - singularity
      prefer:
      - bridges2
      require:
      - pulsar
      - bridges2
      # Maintenance October 8-10 2024
      reject:
      - offline

  bridges2_em:
    runner: bridges2
    min_accepted_cores: 24
    min_accepted_mem: 1024
    max_accepted_cores: 96
    max_accepted_mem: 4096
    context:
      time: 72:00:00
    params:
      #submit_native_specification: "--partition=EM --time={time} --nodes=1 --ntasks-per-node={int(cores - (cores % 24))}"
      submit_native_specification: "--partition=EM --time={time} --nodes=1 --ntasks={int(cores - (cores % 24))}"
      tmp_dir: true
      outputs_to_working_directory: false
      metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{int(cores - (cores % 24))}"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - hpc
      - cvmfs
      - singularity
      require:
      - bridges2-em
      - vgp
      - pulsar
      reject:
      - offline

  expanse:
    runner: expanse
    min_accepted_cores: 32
    min_accepted_mem: 64
    max_accepted_cores: 128
    max_accepted_mem: 512
    max_mem: 500
    context:
      time: 48:00:00
    params:
      submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org oasis.opensciencegrid.org -- /cvmfs/oasis.opensciencegrid.org/mis/apptainer/bin/apptainer
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    # Expanse has some aggressive scratch cleanup, txn especially gets quickly nuked
    - execute: for d in $(seq 0 255 | while read n; do printf "%02x " "$n"; done) txn quarantaine; do mkdir -p /expanse/lustre/scratch/xgalaxy/temp_project/cvmfs_cache/$d; done
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    - name: GALAXY_SLOTS
      value: "{cores}"
      #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # Expanse disabled user namespaces at some point
    #- name: CVMFSEXEC_PATH
    #  value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    #- execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: CVMFSEXEC_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem*1024)}"
    scheduling:
      accept:
      - expanse
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - expanse
      require:
      - pulsar
      - expanse
      #reject:
      #- offline
