---

destinations:

  # abstract envs for inheritance

  _tacc_hpc:
    abstract: true
    context:
      time: 24:00:00
    params:
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      # the tacc-apptainer module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR

  _stampede2:
    inherits: _tacc_hpc
    abstract: true
    params:
      file_action_config: {{ galaxy_config_dir }}/pulsar_stampede_actions.yml
    env:
    {# FIXME: why is this not set in jobs? is it not being read from system profile or something? also, pretty sure it worked fine once without this in job 1464700 but not again? -#}
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    {# mount is in /work2 so the mountpoint (which will appear untouched/empty to tacc cleanup scripts) won't be removed, but also needs to be automatically mounted into singularity containers (which /run is not) -#}
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede2
    - name: CVMFS_MOUNT_ROOT
      value: $WORK2/cvmfs
    - name: CVMFS_ALIEN_CACHE
      value: $SCRATCH/cvmfs
    - name: CVMFS_BINARY
      value: $HOME/cvmfs/bin/cvmfs2.wrap
    - execute: /bin/bash /home1/03166/xcgalaxy/bin/mount_cvmfs data.galaxyproject.org
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: SINGULARITY_CACHEDIR
      value: /work2/03166/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"

  # real envs

  roundup:
    runner: slurm
    max_accepted_cores: 6
    max_accepted_mem: 64
    context:
      partition: multi
      time: 24:00:00
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={partition}"
      tmp_dir: true
      outputs_to_working_directory: true
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    scheduling:
      accept:
      - roundup
      - general
      - cvmfs

  jetstream2:
    runner: jetstream2
    max_accepted_cores: 32
    max_accepted_mem: 125
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition=tpv"
      tmp_dir: true
      outputs_to_working_directory: false
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
    env:
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    scheduling:
      accept:
      - jetstream2
      - general
      - cvmfs
      prefer:
      - jetstream2
      require:
      - pulsar

  bridges2:
    runner: bridges
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--partition=RM --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks-per-node={int(mem/2)}"
    scheduling:
      accept:
      - bridges2
      - hpc
      prefer:
      - bridges2
      require:
      - pulsar

  expanse:
    runner: expanse
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time} --nodes=1 --ntasks={cores}"
      file_action_config: {{ galaxy_config_dir }}/pulsar_expanse_actions.yml
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    - name: GALAXY_SLOTS
      value: "{cores}"
      #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFS_MOUNT_ROOT
      value: /expanse/projects/qstore/pen160/xgalaxy/cvmfs
    - name: CVMFS_ALIEN_CACHE
      value: /expanse/lustre/scratch/xgalaxy/temp_project/cvmfs_cache
    - execute: /bin/bash $HOME/bin/mount_cvmfs data.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={mem*1024}"
    scheduling:
      accept:
      - expanse
      - hpc
      - cvmfs
      prefer:
      - expanse
      require:
      - pulsar

  rockfish:
    runner: rockfish
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--partition=defq --time={time} --nodes=1 --ntasks-per-node={cores} --mem={mem*1024}"
      file_action_config: {{ galaxy_config_dir }}/pulsar_rockfish_actions.yml
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/nekrut/galaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish | join(',') }}"
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    #- name: GALAXY_MEMORY_MB
    #  value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/galaxy_job_$SLURM_JOB_ID
    - execute: mkdir $TRINITY_SCRATCH_DIR
    - execute: trap "rm -rf $TRINITY_SCRATCH_DIR" EXIT
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # FATAL:   singularity image is not in an allowed configured path
    #- name: SINGULARITY_CACHEDIR
    #  value: /scratch4/nekrut/galaxy/singularity_cache
    #- name: SINGULARITY_PYTHREADS
    #  value: "9"
    scheduling:
      accept:
      - rockfish
      - hpc
      prefer:
      - rockfish
      require:
      - pulsar
      reject:
      - cvmfs
      - offline

  frontera:
    inherits: _tacc_hpc
    runner: frontera
    max_accepted_cores: 56
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=small"
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - frontera
      - hpc
      - cvmfs
      prefer:
      - frontera
      require:
      - pulsar
      reject:
      - offline

  stampede2_knl:
    inherits: _stampede2
    runner: stampede
    max_accepted_cores: 272
    max_accepted_mem: 94
    context:
      partition: normal
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition={partition}"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "94000"
    scheduling:
      accept:
      - stampede2
      # removing from hpc for now to burn some SUs on other systems and avoid the limit hole
      #- hpc
      - cvmfs
      prefer:
      - stampede2-knl
      require:
      - pulsar

  stampede2_skx:
    inherits: _stampede2
    runner: stampede
    max_accepted_cores: 96
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=skx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      prefer:
      - stampede2-skx
      require:
      - pulsar
      # apptainer switch has broken containers
      reject:
      - offline

  stampede2_icx:
    inherits: _stampede2
    runner: stampede
    max_accepted_cores: 160
    max_accepted_mem: 254
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=icx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "254000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      prefer:
      - stampede2-icx
      require:
      - pulsar
      # apptainer switch has broken containers
      reject:
      - offline

  # dev

  stampede2_knl_dev:
    inherits: stampede2_knl
    context:
      partition: development
      time: 01:00:00
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      prefer:
      - stampede2-knl
      require:
      - dev
      - pulsar
