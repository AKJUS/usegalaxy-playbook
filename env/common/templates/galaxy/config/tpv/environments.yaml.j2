---

destinations:

  # abstract envs for inheritance

  _roundup_common:
    abstract: true
    runner: slurm
    #context:
    #  roundup_*_partition and time are set on role and (default) tool entitites
    params:
      tmp_dir: true
      outputs_to_working_directory: true
      metadata_strategy: extended
    env:
    - execute: ulimit -c 0
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100

  _tacc_hpc:
    abstract: true
    context:
      time: 24:00:00
    params:
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.tacc_hpc | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR

  _stampede3:
    inherits: _tacc_hpc
    abstract: true
    params:
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    {# FIXME: why is this not set in jobs? is it not being read from system profile or something? also, pretty sure it worked fine once without this in job 1464700 but not again? -#}
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede3
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"

  # real envs

  roundup_conda_direct:
    inherits: _roundup_common
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_small_partition}"
    scheduling:
      accept:
      - roundup
      - cvmfs
      - training
      require:
      - conda

  roundup:
    inherits: _roundup_common
    min_accepted_cores: 2
    min_accepted_mem: 8
    max_accepted_cores: 12
    max_accepted_mem: 48
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_large_partition} {roundup_constraint}"
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    scheduling:
      accept:
      - roundup
      - general
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker

  roundup_small:
    inherits: roundup
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 2
    max_accepted_mem: 16
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_small_partition} {roundup_constraint}"

  roundup_mem:
    inherits: roundup
    # this is only for pulsar-incompatible jobs, so requires an explicit tag
    min_accepted_cores: 1
    min_accepted_mem: 1
    max_accepted_cores: 32
    max_accepted_mem: 256
    max_cores: 8
    max_mem: 96
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_large_partition} {roundup_constraint}"
    scheduling:
      require:
      - roundup-mem

  # *_training destinations are mainly for reporting, context is set in the role entity
  roundup_training:
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 64
    inherits: roundup
    scheduling:
      require:
      - training
      accept:
      - roundup-mem

  roundup_small_training:
    inherits: roundup_small
    scheduling:
      require:
      - training

  jetstream2:
    runner: jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 248
    context:
      time: 36:00:00
      # js2_partition is set on role and (default) tool entitites
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={js2_partition}"
      tmp_dir: true
      outputs_to_working_directory: false
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
      remote_metadata: true
      remote_property_galaxy_home: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
    env:
    - execute: ulimit -c 0
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: XDG_CACHE_HOME
      value: /tmp/{{ galaxy_user }}-cache
    - name: TERM
      value: vt100
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_XDG_CACHE_HOME
      value: $XDG_CACHE_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    scheduling:
      accept:
      - jetstream2
      - general
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - jetstream2
      require:
      - pulsar
      #reject:
      #- offline

  jetstream2_gpu:
    inherits: jetstream2
    max_accepted_cores: 32
    max_accepted_gpus: 1.0
    max_accepted_mem: 125
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={js2_gpu_partition}"
      singularity_run_extra_arguments: --nv -B /etc/OpenCL/vendors:/usr/local/etc/OpenCL/vendors
    env:
    # gmx_sim uses this, maybe others?
    - name: SINGULARITYENV_GPU_AVAILABLE
      value: "1"
    - execute: nvidia-modprobe -u -c=0
    scheduling:
      require:
      - gpu
      - pulsar
      #reject:
      #- offline

  jetstream2_resize_shm:
    inherits: jetstream2
    params:
      submit_native_specification: "--nodes=1 --time={time} --partition=resize-shm"
    scheduling:
      require:
      - resize-shm
      - jetstream2
      - pulsar
      #reject:
      #- offline

  jetstream2_training:
    inherits: jetstream2
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 64
    max_mem: 250
    scheduling:
      require:
      - training
      #reject:
      #- offline

  jetstream2_k8s:
    runner: jetstream2_k8s
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 120
    params:
      docker_enabled: true  # probably shouldn't be needed but is still
      outputs_to_working_directory: false
      container_resolvers:
        - type: explicit
      docker_default_container_id: 'quay.io/biocontainers/python:3.6.7'
      #pulsar_container_image: '{{ pulsar_coexecution_container_image | default("galaxy/pulsar-pod-staging:0.15.0.1") }}'
      #k8s_namespace: ndc
      #k8s_walltime_limit: 86400  # 24 hours
      k8s_walltime_limit: 43200  # 12 hours
      pulsar_requests_cpu: 0.5
      pulsar_requests_memory: 0.5Gi
      pulsar_limits_cpu: 0.5
      pulsar_limits_memory: 0.5Gi
      tool_requests_cpu: 1.5
      tool_requests_memory: 2.5Gi
      tool_limits_cpu: 1.5
      tool_limits_memory: 2.5Gi
      #jobs_directory: /not/a/real/path
      pulsar_app_config_path: {{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml
      # Specify a non-default Pulsar staging container.
      # Generate job names with a string unique to this Galaxy (see
      # Kubernetes runner description).
      #k8s_galaxy_instance_id: mycoolgalaxy
      # Path to Kubernetes configuration fil (see Kubernetes runner description.)
      k8s_config_path: /home/{{ galaxy_user }}/.kube/config
    scheduling:
      accept:
      - pulsar
      - training
      require:
      - kubernetes
      #reject:
      #- offline

  bridges2:
    runner: bridges
    min_accepted_cores: 16
    min_accepted_mem: 64
    # TODO: temp for stampede3 maint 2024/10/15
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 128
    max_mem: 250
    # Force large jobs to Stampede3 to preserve credits on shared resources
    #max_accepted_cores: 64
    #max_accepted_mem: 128
    #max_cores: 64
    #max_mem: 128
    context:
      time: 36:00:00
    params:
      submit_native_specification: "--partition=RM --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{min(int(mem/2), 64)}"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem/2)*2000}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        # https://github.com/natefoo/slurm-drmaa/issues/83
        #submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks-per-node={int(mem/2)} --mem={int(mem/2)*2000}"
        submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem/2)*2000}"
    scheduling:
      accept:
      - bridges2
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - bridges2
      require:
      - pulsar
      #reject:
      #- offline

  expanse:
    runner: expanse
    min_accepted_cores: 16
    min_accepted_mem: 64
    max_accepted_cores: 64
    max_accepted_mem: 128
    #max_cores: 128
    #max_mem: 250
    context:
      time: 36:00:00
    params:
      submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org oasis.opensciencegrid.org -- /cvmfs/oasis.opensciencegrid.org/mis/apptainer/bin/apptainer
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    # Expanse has some aggressive scratch cleanup, txn especially gets quickly nuked
    - execute: for d in $(seq 0 255 | while read n; do printf "%02x " "$n"; done) txn quarantaine; do mkdir -p /expanse/lustre/scratch/xgalaxy/temp_project/cvmfs_cache/$d; done
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    - name: GALAXY_SLOTS
      value: "{cores}"
      #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # Expanse disabled user namespaces at some point
    #- name: CVMFSEXEC_PATH
    #  value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    #- execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: CVMFSEXEC_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem*1024)}"
    scheduling:
      accept:
      - expanse
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - expanse
      require:
      - pulsar
      #reject:
      #- offline

  _rockfish:
    abstract: true
    runner: rockfish
    params:
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/nekrut/galaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish | join(',') }}"
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{int(mem/4)}"
    #- name: GALAXY_MEMORY_MB
    #  value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/galaxy_job_$SLURM_JOB_ID
    - execute: mkdir $TRINITY_SCRATCH_DIR
    - execute: trap "rm -rf $TRINITY_SCRATCH_DIR" EXIT
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - execute: module load singularity/3.8.7
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    # set in module
    #- name: SINGULARITY_CACHEDIR
    #  value: /scratch4/nekrut/galaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"

  rockfish:
    inherits: _rockfish
    #min_accepted_cores: 16
    min_accepted_cores: 8
    min_accepted_mem: 64
    #max_accepted_cores: 48
    #max_accepted_mem: 192
    # Force large jobs to Stampede3 to preserve credits on shared resources
    max_accepted_cores: 32
    max_accepted_mem: 128
    max_cores: 48
    max_mem: 184
    context:
      account: nekrut
    params:
      # https://github.com/natefoo/slurm-drmaa/issues/83
      #submit_native_specification: "--account={account} --partition={partition} --time={time} --nodes=1 --ntasks-per-node={cores} --mem={int(mem*1024)}"
      # max time on shared is 24
      submit_native_specification: "--account={account} --partition=shared --time=24:00:00 --nodes=1 --ntasks={int(mem/4)} --mem={int(mem*1024)}"
      tmp_dir: true
      outputs_to_working_directory: false
    rules:
    - if: cores > 32 or mem > 128
      params:
        submit_native_specification: "--account={account} --partition=parallel --time=36:00:00 --nodes=1 --ntasks={cores} --mem={int(mem*1024)}"
    scheduling:
      accept:
      - rockfish
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - rockfish
      require:
      - pulsar
      # expired? 2024-06-13
      reject:
      - offline

  rockfish_gpu:
    inherits: _rockfish
    min_accepted_cores: 12
    min_accepted_mem: 48
    context:
      account: mschatz1_gpu
      partition: a100
      time: 24:00:00
    params:
      # https://github.com/natefoo/slurm-drmaa/issues/83
      #submit_native_specification: "--account={account} --partition={partition} --gres=gres:gpu:{gpus} --time={time} --nodes=1 --ntasks-per-node={cores} --mem-per-cpu=4096"
      submit_native_specification: "--account={account} --partition={partition} --time={time} --nodes=1 --ntasks={cores} --gres=gres:gpu:{gpus}"
      singularity_run_extra_arguments: --nv
    scheduling:
      accept:
      - rockfish
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      require:
      - pulsar
      - rockfish-gpu

  rockfish_devgalaxy:
    runner: rockfish_devgalaxy
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 1
    max_accepted_mem: 4
    params:
      submit_native_specification: "--account=mschatz1 --partition=shared --time=12:00:00 --nodes=1 --ntasks={int(mem/4)} --mem={int(mem*1024)}"
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/mschatz1/galaxy/rockfish-devgalaxy/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish_devgalaxy | join(',') }}"
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      require_container: true
      tmp_dir: true
      outputs_to_working_directory: false
    env:
    - name: PATH
      value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{int(mem/4)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - execute: module load singularity/3.8.7
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - singularity
      require:
      - rockfish-devgalaxy

  frontera:
    inherits: _tacc_hpc
    runner: frontera
    min_accepted_cores: 16
    min_accepted_mem: 96
    max_accepted_cores: 56
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=small"
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - frontera
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - frontera
      require:
      - pulsar
      reject:
      - offline

  frontera_rtx:
    runner: frontera
    min_accepted_cores: 1
    min_accepted_mem: 2
    max_accepted_cores: 16
    max_accepted_mem: 120
    context:
      account: BIR23002
      partition: rtx
    params:
      submit_native_specification: "--account={account} --nodes=1 --ntasks={cores} --time={time} --partition={partition}"
      singularity_run_extra_arguments: --nv
      # _tacc_hpc params
      outputs_to_working_directory: false
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      # the tacc-apptainer module automatically sets up mounts
      singularity_volumes: null
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: mulled_singularity
      require_container: true
    scheduling:
      accept:
      - cvmfs
      - singularity
      require:
      - frontera-rtx
      - pulsar
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS

  stampede3_skx:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 16
    min_accepted_mem: 128
    max_accepted_cores: 64
    max_accepted_mem: 190
    max_cores: 48
    max_mem: 184
    params:
      # $GALAXY_SLOTS is forced to 48 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=skx"
      submit_native_specification: "--nodes=1 --time={time} --partition=skx"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_SLOTS
      value: 48
    - name: GALAXY_MEMORY_MB
      value: "188000"
    scheduling:
      accept:
      - stampede3
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede3-skx
      require:
      - pulsar
      reject:
      - offline

  stampede3_icx:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 32
    min_accepted_mem: 190
    max_accepted_cores: 80
    max_accepted_mem: 4096
    max_cores: 80
    max_mem: 248
    params:
      # $GALAXY_SLOTS is forced to 64 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=icx"
      submit_native_specification: "--nodes=1 --time={time} --partition=icx"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_SLOTS
      value: 80
    - name: GALAXY_MEMORY_MB
      value: "248000"
    scheduling:
      accept:
      - stampede3
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede3-icx
      require:
      - pulsar
      reject:
      - offline

  stampede3_spr:
    inherits: _stampede3
    runner: stampede3
    min_accepted_cores: 112
    min_accepted_mem: 96
    max_accepted_cores: 256
    max_accepted_mem: 128
    max_cores: 112
    max_mem: 120
    params:
      # $GALAXY_SLOTS is forced to 112 to avoid wastage
      #submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=icx"
      submit_native_specification: "--nodes=1 --time={time} --partition=spr"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_SLOTS
      value: 112
    - name: GALAXY_MEMORY_MB
      value: "122880"
    scheduling:
      accept:
      - stampede3
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede3-spr
      require:
      - pulsar
      reject:
      - offline
