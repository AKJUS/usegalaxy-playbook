---

destinations:

  # abstract envs for inheritance

  _roundup_common:
    abstract: true
    runner: slurm
    #context:
    #  roundup_*_partition and time are set on role and (default) tool entitites
    params:
      tmp_dir: true
      outputs_to_working_directory: true
      metadata_strategy: extended
    env:
    - execute: ulimit -c 0
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C

  _tacc_hpc:
    abstract: true
    context:
      time: 36:00:00
    params:
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      # the tacc-apptainer module automatically sets up mounts
      singularity_volumes: /cvmfs/data.galaxyproject.org:ro
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR

  _stampede2:
    inherits: _tacc_hpc
    abstract: true
    params:
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    {# FIXME: why is this not set in jobs? is it not being read from system profile or something? also, pretty sure it worked fine once without this in job 1464700 but not again? -#}
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    {# mount is in /work2 so the mountpoint (which will appear untouched/empty to tacc cleanup scripts) won't be removed, but also needs to be automatically mounted into singularity containers (which /run is not) -#}
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede2
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    #- name: CVMFS_MOUNT_ROOT
    #  value: $WORK2/cvmfs
    #- name: CVMFS_ALIEN_CACHE
    #  value: $SCRATCH/cvmfs
    #- name: CVMFS_BINARY
    #  value: $HOME/cvmfs/bin/cvmfs2.wrap
    #- execute: /bin/bash /home1/03166/xcgalaxy/bin/mount_cvmfs data.galaxyproject.org
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"

  # real envs

  roundup_conda_direct:
    inherits: _roundup_common
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 4
    max_accepted_mem: 32
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_small_partition}"
    scheduling:
      accept:
      - roundup
      - cvmfs
      - training
      require:
      - conda

  roundup:
    inherits: _roundup_common
    min_accepted_cores: 2
    min_accepted_mem: 16
    max_accepted_cores: 8
    max_accepted_mem: 64
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_large_partition}"
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    scheduling:
      accept:
      - roundup
      - general
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker

  roundup_small:
    inherits: roundup
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 2
    max_accepted_mem: 16
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_small_partition}"

  roundup_mem:
    inherits: roundup
    # this is only for pulsar-incompatible jobs, so requires an explicit tag
    min_accepted_cores: 1
    min_accepted_mem: 1
    max_accepted_cores: 32
    max_accepted_mem: 256
    max_cores: 8
    max_mem: 96
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={roundup_large_partition}"
    scheduling:
      require:
      - roundup-mem

  # *_training destinations are mainly for reporting, context is set in the role entity
  roundup_training:
    inherits: roundup
    scheduling:
      require:
      - training

  roundup_small_training:
    inherits: roundup_small
    scheduling:
      require:
      - training

  jetstream2:
    runner: jetstream2
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 120
    context:
      time: 36:00:00
      # js2_partition is set on role and (default) tool entitites
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={js2_partition}"
      tmp_dir: true
      outputs_to_working_directory: false
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
      remote_metadata: true
      remote_property_galaxy_home: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
    env:
    - execute: ulimit -c 0
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    scheduling:
      accept:
      - jetstream2
      - general
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - jetstream2
      require:
      - pulsar

  jetstream2_gpu:
    inherits: jetstream2
    max_accepted_cores: 32
    max_accepted_gpus: 1.0
    max_accepted_mem: 125
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition=gpu-small"
      singularity_run_extra_arguments: --nv -B /etc/OpenCL/vendors:/usr/local/etc/OpenCL/vendors
    env:
    # gmx_sim uses this, maybe others?
    - name: SINGULARITYENV_GPU_AVAILABLE
      value: "1"
    scheduling:
      require:
      - gpu
      - pulsar

  jetstream2_resize_shm:
    inherits: jetstream2
    params:
      submit_native_specification: "--nodes=1 --time={time} --partition=resize-shm"
    scheduling:
      require:
      - resize-shm
      - jetstream2
      - pulsar

  jetstream2_training:
    inherits: jetstream2
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 64
    max_mem: 250
    scheduling:
      require:
      - training

  jetstream2_k8s:
    runner: jetstream2_k8s
    min_accepted_cores: 1
    min_accepted_mem: 0
    max_accepted_cores: 32
    max_accepted_mem: 120
    params:
      docker_enabled: true  # probably shouldn't be needed but is still
      outputs_to_working_directory: false
      container_resolvers:
        - type: explicit
      docker_default_container_id: 'quay.io/biocontainers/python:3.6.7'
      #pulsar_container_image: '{{ pulsar_coexecution_container_image | default("galaxy/pulsar-pod-staging:0.15.0.1") }}'
      #k8s_namespace: ndc
      #k8s_walltime_limit: 86400  # 24 hours
      k8s_walltime_limit: 43200  # 12 hours
      pulsar_requests_cpu: 0.5
      pulsar_requests_memory: 0.5Gi
      pulsar_limits_cpu: 0.5
      pulsar_limits_memory: 0.5Gi
      tool_requests_cpu: 1.5
      tool_requests_memory: 1.5Gi
      tool_limits_cpu: 1.5
      tool_limits_memory: 1.5Gi
      #jobs_directory: /not/a/real/path
      pulsar_app_config_path: {{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml
      # Specify a non-default Pulsar staging container.
      # Generate job names with a string unique to this Galaxy (see
      # Kubernetes runner description).
      #k8s_galaxy_instance_id: mycoolgalaxy
      # Path to Kubernetes configuration fil (see Kubernetes runner description.)
      k8s_config_path: /home/{{ galaxy_user }}/.kube/config
    scheduling:
      accept:
      - pulsar
      - training
      require:
      - kubernetes

  bridges2:
    runner: bridges
    min_accepted_cores: 16
    min_accepted_mem: 64
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 128
    max_mem: 250
    context:
      time: 36:00:00
    params:
      submit_native_specification: "--partition=RM --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks-per-node={int(mem/2)}"
    scheduling:
      accept:
      - bridges2
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - bridges2
      require:
      - pulsar

  expanse:
    runner: expanse
    #min_accepted_cores: 16
    min_accepted_cores: 8
    min_accepted_mem: 64
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 128
    max_mem: 250
    context:
      time: 36:00:00
    params:
      submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org oasis.opensciencegrid.org -- /cvmfs/oasis.opensciencegrid.org/mis/apptainer/bin/apptainer
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    - name: GALAXY_SLOTS
      value: "{cores}"
      #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # Expanse disabled user namespaces at some point
    #- name: CVMFSEXEC_PATH
    #  value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    #- execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: CVMFSEXEC_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={int(mem*1024)}"
    scheduling:
      accept:
      - expanse
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - expanse
      require:
      - pulsar
      # TODO: why is it not really processing jobs?
      #reject:
      #- offline

  rockfish:
    runner: rockfish
    #min_accepted_cores: 16
    min_accepted_cores: 8
    min_accepted_mem: 64
    max_accepted_cores: 48
    max_accepted_mem: 192
    max_cores: 48
    max_mem: 184
    context:
      account: nekrut
    params:
      # https://github.com/natefoo/slurm-drmaa/issues/83
      #submit_native_specification: "--account={account} --partition={partition} --time={time} --nodes=1 --ntasks-per-node={cores} --mem={int(mem*1024)}"
      # max time on shared is 24
      submit_native_specification: "--account={account} --partition=shared --time=24:00:00 --nodes=1 --ntasks={cores} --mem={int(mem*1024)}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: {{ pulsar_default_file_action | default("remote_transfer") }}
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/nekrut/galaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish | join(',') }}"
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    #- name: GALAXY_MEMORY_MB
    #  value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/galaxy_job_$SLURM_JOB_ID
    - execute: mkdir $TRINITY_SCRATCH_DIR
    - execute: trap "rm -rf $TRINITY_SCRATCH_DIR" EXIT
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - execute: module load singularity/3.8.7
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    # set in module
    #- name: SINGULARITY_CACHEDIR
    #  value: /scratch4/nekrut/galaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: cores > 32 or mem > 128
      params:
        submit_native_specification: "--account={account} --partition=parallel --time=36:00:00 --nodes=1 --ntasks={cores} --mem={int(mem*1024)}"
    scheduling:
      accept:
      - rockfish
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - rockfish
      require:
      - pulsar

  rockfish_gpu:
    inherits: rockfish
    min_accepted_cores: 12
    min_accepted_mem: 48
    context:
      account: mschatz1_gpu
      partition: a100
      time: 24:00:00
    params:
      # https://github.com/natefoo/slurm-drmaa/issues/83
      #submit_native_specification: "--account={account} --partition={partition} --gres=gres:gpu:{gpus} --time={time} --nodes=1 --ntasks-per-node={cores} --mem-per-cpu=4096"
      submit_native_specification: "--account={account} --partition={partition} --time={time} --nodes=1 --ntasks={cores} --gres=gres:gpu:{gpus}"
      singularity_run_extra_arguments: --nv
    scheduling:
      require:
      - rockfish-gpu

  frontera:
    inherits: _tacc_hpc
    runner: frontera
    min_accepted_cores: 16
    min_accepted_mem: 96
    max_accepted_cores: 56
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=small"
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - frontera
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - frontera
      require:
      - pulsar
      reject:
      - offline

  stampede2_skx:
    inherits: _stampede2
    runner: stampede
    min_accepted_cores: 16
    min_accepted_mem: 96
    max_accepted_cores: 96
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=skx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede2-skx
      require:
      - pulsar
      reject:
      - offline

  stampede2_icx:
    inherits: _stampede2
    runner: stampede
    min_accepted_cores: 32
    min_accepted_mem: 128
    max_accepted_cores: 256
    max_accepted_mem: 4096
    max_cores: 160
    max_mem: 250
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=icx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "254000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      - singularity
      # for TPV shared DB
      - docker
      prefer:
      - stampede2-icx
      require:
      - pulsar
      reject:
      - offline
