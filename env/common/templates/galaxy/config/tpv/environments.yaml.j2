---

destinations:

  # abstract envs for inheritance

  _tacc_hpc:
    abstract: true
    context:
      time: 24:00:00
    params:
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      # the tacc-apptainer module automatically sets up mounts
      singularity_volumes: /cvmfs/data.galaxyproject.org:ro
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - file: /etc/profile.d/z01_lmod.sh
    - execute: module unload xalt
    - execute: module load tacc-apptainer
    - name: GALAXY_SLOTS
      value: "$SLURM_NTASKS"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR

  _stampede2:
    inherits: _tacc_hpc
    abstract: true
    params:
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- apptainer
    env:
    {# FIXME: why is this not set in jobs? is it not being read from system profile or something? also, pretty sure it worked fine once without this in job 1464700 but not again? -#}
    - name: SCRATCH
      value: /scratch/03166/xcgalaxy
    {# mount is in /work2 so the mountpoint (which will appear untouched/empty to tacc cleanup scripts) won't be removed, but also needs to be automatically mounted into singularity containers (which /run is not) -#}
    - name: WORK2
      value: /work2/03166/xcgalaxy/stampede2
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    #- name: CVMFS_MOUNT_ROOT
    #  value: $WORK2/cvmfs
    #- name: CVMFS_ALIEN_CACHE
    #  value: $SCRATCH/cvmfs
    #- name: CVMFS_BINARY
    #  value: $HOME/cvmfs/bin/cvmfs2.wrap
    #- execute: /bin/bash /home1/03166/xcgalaxy/bin/mount_cvmfs data.galaxyproject.org
    {# FIXME: why is this not set by module load tacc-apptainer? -#}
    - name: APPTAINER_CACHEDIR
      value: /work2/03166/xcgalaxy/apptainer_cache
    - name: APPTAINER_PYTHREADS
      value: "9"

  # real envs

  roundup:
    runner: slurm
    max_accepted_cores: 6
    max_accepted_mem: 64
    context:
      partition: multi
      time: 24:00:00
    params:
      native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={partition}"
      tmp_dir: true
      outputs_to_working_directory: true
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.local | join(',') }}"
      singularity_no_mount: null
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
    env:
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    scheduling:
      accept:
      - roundup
      - general
      - cvmfs

  jetstream2:
    runner: jetstream2
    max_accepted_cores: 32
    max_accepted_mem: 125
    context:
      time: 24:00:00
      partition: tpv
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --mem={round(mem*1024)} --time={time} --partition={partition}"
      tmp_dir: true
      outputs_to_working_directory: false
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.jetstream | join(',') }}"
      singularity_default_container_id: "{{ galaxy_job_conf_default_container_id }}"
      singularity_no_mount: null
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
    env:
    - execute: ulimit -c 0
    # SINGULARITYENV_TMP=$TMP and SINGULARITYENV_TMPDIR=$TMPDIR are set on the singularity command line
    - name: TMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TEMP
      value: $_GALAXY_JOB_TMP_DIR
    - name: TMPDIR
      value: $_GALAXY_JOB_TMP_DIR
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
    - name: XDG_DATA_HOME
      value: /cvmfs/{{ galaxy_cvmfs_repo }}/xdg/data
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: SINGULARITYENV_XDG_DATA_HOME
      value: $XDG_DATA_HOME
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    scheduling:
      accept:
      - jetstream2
      - general
      - cvmfs
      prefer:
      - jetstream2
      require:
      - pulsar

  jetstream2_gpu:
    inherits: jetstream2
    max_accepted_cores: 32
    max_accepted_gpus: 1.0
    max_accepted_mem: 125
    context:
      partition: gpu-small
    params:
      singularity_run_extra_arguments: --nv -B /etc/OpenCL/vendors:/usr/local/etc/OpenCL/vendors
    env:
    # gmx_sim uses this, maybe others?
    - name: SINGULARITYENV_GPU_AVAILABLE
      value: "1"
    scheduling:
      require:
      - gpu
      - pulsar

  bridges2:
    runner: bridges
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--partition=RM --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.bridges | join(',') }}"
      singularity_no_mount: null
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    - name: GALAXY_MEMORY_MB
      value: "{int(mem*1024)}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: $LOCAL
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - name: CVMFSEXEC_DIR
      value: $(dirname $_GALAXY_JOB_DIR)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /ocean/projects/mcb140028p/xcgalaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--partition=RM-shared --time={time} --nodes=1 --ntasks-per-node={int(mem/2)}"
    scheduling:
      accept:
      - bridges2
      - hpc
      - cvmfs
      prefer:
      - bridges2
      require:
      - pulsar

  expanse:
    runner: expanse
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--account=TG-MCB140147 --partition=compute --time={time} --nodes=1 --ntasks={cores}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.expanse | join(',') }}"
      #singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org oasis.opensciencegrid.org -- /cvmfs/oasis.opensciencegrid.org/mis/apptainer/bin/apptainer
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - file: /etc/profile.d/00-sdsc-modules.sh
    - execute: module load slurm
    - execute: module load singularitypro
    - name: GALAXY_SLOTS
      value: "{cores}"
      #value: "$SLURM_NTASKS"
    - name: GALAXY_MEMORY_MB
      value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    # Expanse disabled user namespaces at some point
    #- name: CVMFSEXEC_PATH
    #  value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    #- execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    - name: CVMFSEXEC_DIR
      value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_DIR/cvmfsexec"
    # run once to extract
    - execute: $CVMFSEXEC_DIR/cvmfsexec -v >/dev/null
    - execute: trap "$CVMFSEXEC_DIR/.cvmfsexec/umountrepo -a" EXIT
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo data.galaxyproject.org
    - execute: $CVMFSEXEC_DIR/.cvmfsexec/mountrepo singularity.galaxyproject.org
    - name: SINGULARITY_CACHEDIR
      value: /expanse/lustre/scratch/xgalaxy/temp_project/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    rules:
    - if: mem <= 128
      params:
        submit_native_specification: "--account=TG-MCB140147 --partition=shared --time={time} --nodes=1 --ntasks={int(mem/2)} --mem={mem*1024}"
    scheduling:
      accept:
      - expanse
      - hpc
      - cvmfs
      prefer:
      - expanse
      require:
      - pulsar

  rockfish:
    runner: rockfish
    max_accepted_cores: 128
    max_accepted_mem: 256
    context:
      time: 24:00:00
    params:
      submit_native_specification: "--partition=defq --time={time} --nodes=1 --ntasks-per-node={cores} --mem={mem*1024}"
      tmp_dir: true
      outputs_to_working_directory: false
      #metadata_strategy: extended
      remote_metadata: false
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: local
      rewrite_parameters: true
      jobs_directory: /scratch4/nekrut/galaxy/{{ galaxy_instance_codename }}/staging
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes.rockfish | join(',') }}"
      singularity_cmd: $CVMFSEXEC_PATH -N data.galaxyproject.org singularity.galaxyproject.org -- singularity
      singularity_no_mount: null
      container_resolvers:
      - type: explicit_singularity
      - type: cached_mulled_singularity
        cache_directory: /cvmfs/singularity.galaxyproject.org/all
        cache_directory_cacher_type: dir_mtime
      - type: mulled_singularity
      require_container: true
    env:
    - name: PATH
      value: /cm/shared/apps/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
    - name: GALAXY_SLOTS
      value: "{cores}"
    #- name: GALAXY_MEMORY_MB
    #  value: "{mem*1024}"
    - execute: ulimit -c 0
    - name: _JAVA_OPTIONS
      value: -Djava.io.tmpdir=$TEMP
    - name: TERM
      value: vt100
    - name: LC_ALL
      value: C
    - name: TRINITY_SCRATCH_DIR
      value: /tmp/galaxy_job_$SLURM_JOB_ID
    - execute: mkdir $TRINITY_SCRATCH_DIR
    - execute: trap "rm -rf $TRINITY_SCRATCH_DIR" EXIT
    - name: SINGULARITYENV_TERM
      value: $TERM
    - name: SINGULARITYENV__JAVA_OPTIONS
      value: $_JAVA_OPTIONS
    - name: SINGULARITYENV_LC_ALL
      value: $LC_ALL
    - name: SINGULARITYENV_TEMP
      value: $TEMP
    - name: SINGULARITYENV_TMPDIR
      value: $TEMP
    - name: SINGULARITYENV_TRINITY_SCRATCH_DIR
      value: $TRINITY_SCRATCH_DIR
    - execute: module load singularity/3.8.7
    - name: CVMFSEXEC_PATH
      value: $(readlink -f $_GALAXY_JOB_DIR/../cvmfsexec)
    - execute: cp "$HOME/bin/cvmfsexec" "$CVMFSEXEC_PATH"
    # set in module
    #- name: SINGULARITY_CACHEDIR
    #  value: /scratch4/nekrut/galaxy/singularity_cache
    - name: SINGULARITY_PYTHREADS
      value: "9"
    scheduling:
      accept:
      - rockfish
      - hpc
      - cvmfs
      prefer:
      - rockfish
      require:
      - pulsar

  frontera:
    inherits: _tacc_hpc
    runner: frontera
    max_accepted_cores: 56
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=small"
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - frontera
      - hpc
      - cvmfs
      prefer:
      - frontera
      require:
      - pulsar
      reject:
      - offline

  stampede2_skx:
    inherits: _stampede2
    runner: stampede
    max_accepted_cores: 96
    max_accepted_mem: 190
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=skx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "190000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      prefer:
      - stampede2-skx
      require:
      - pulsar

  stampede2_icx:
    inherits: _stampede2
    runner: stampede
    max_accepted_cores: 160
    max_accepted_mem: 254
    params:
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --time={time} --partition=icx-normal"
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
    env:
    - name: GALAXY_MEMORY_MB
      value: "254000"
    scheduling:
      accept:
      - stampede2
      - hpc
      - cvmfs
      prefer:
      - stampede2-icx
      require:
      - pulsar
