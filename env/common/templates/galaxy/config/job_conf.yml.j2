---
##
## This file is maintained by Ansible - CHANGES WILL BE OVERWRITTEN
##

{#
## WARNING: this file is shared between Test and Main!
##
## NOTE: many template values can be found in env/<env>/group_vars/galaxyservers/tools_conf.yml
-#}

{#-
## template macros
#}

{%- macro runner_pulsar_params() -%}
    amqp_url: {{ galaxy_job_conf_amqp_url }}
    galaxy_url: {{ galaxy_job_conf_pulsar_galaxy_url }}
    persistence_directory: /srv/galaxy/{{ galaxy_instance_codename }}/var/pulsar_amqp_ack
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60
{%- endmacro %}

{%- macro env_local_envs(java_mem=7) -%}
        # cloudmap tools are still using R 2.11(!) from here, also the genome diversity tools use things in /galaxy/software -->
        #- name: PATH
        #  value: /galaxy/{{ galaxy_instance_codename }}/linux-x86_64/bin:/galaxy/software/linux-x86_64/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
        # TODO: what uses this (DejaVu fonts)?
        - name: XDG_DATA_HOME
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/xdg/data
        {{ env_temp_envs() }}
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx{{ java_mem }}g -Xms256m
        - execute: ulimit -c 0
        - name: SINGULARITYENV_LC_ALL
          value: C
{%- endmacro %}

# FIXME: remove
{%- macro env_legacy_envs(java_mem=7) -%}
        - file: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin/activate
        - name: GALAXY_VIRTUAL_ENV
          value: None
        - name: PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/deps/_py2/bin:$PATH
{%- endmacro %}

{%- macro env_pulsar_envs() -%}
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
{%- endmacro %}

{%- macro env_temp_envs() -%}
        - name: $TEMP
          #raw: true
          value: $(dirname ${BASH_SOURCE[0]})/_job_tmp
        - name: SINGULARITYENV_TEMP
          value: $TEMP
        - name: SINGULARITYENV_TMPDIR
          value: $TEMP
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: -Djava.io.tmpdir=$TEMP
        - execute: mkdir -p $TEMP
{%- endmacro %}

{%- macro env_tag_params(tags) -%}
{% if tags is not none -%}
      tags:
{% for tag in tags %}
        - {{ tag }}
{%- endfor -%}
{%- endif -%}
{%- endmacro %}

{%- macro env_pulsar_params(remote_metadata='true') -%}
      remote_metadata: {{ remote_metadata }}
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: remote
      rewrite_parameters: true
      submit_user_email: $__user_email__
      outputs_to_working_directory: false
{%- endmacro %}

{%- macro env_jetstream_params() -%}
      jobs_directory: /jetstream/scratch0/{{ galaxy_instance_codename }}/jobs
      # this doesn't work, set in supervisor environment instead
      #remote_property_galaxy_virtual_env: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      file_action_config: {{ galaxy_config_dir }}/pulsar_jetstream_actions.yml
      outputs_to_working_directory: false
{%- endmacro %}

#
# Job runner plugin configuration
#
runners:
  dynamic:
    # these live in the virtualenv
    rules_module: usegalaxy.jobs.rules
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib64/libdrmaa.so
    invalidjobexception_retries: 5
    internalexception_retries: 5
  jetstream_iu:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream_iu
    {{ runner_pulsar_params() }}
  jetstream_tacc:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream_tacc
    {{ runner_pulsar_params() }}
  stampede:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: stampede
    {{ runner_pulsar_params() }}
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    {{ runner_pulsar_params() }}
  bridges:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: bridges
    {{ runner_pulsar_params() }}
  tacc_k8s:
    load: galaxy.jobs.runners.pulsar:PulsarKubernetesJobRunner
    manager: tacc_k8s
    {{ runner_pulsar_params() }}

#
# Job handler configuration
#
handling:
  #default: handlers
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32
  processes:
{% for handler in galaxy_job_conf_handlers %}
    {{ handler.id }}:
      plugins:
{% for plugin in handler.plugins %}
        - {{ plugin }}
{% endfor %}
      tags:
{% for tag in handler.tags %}
        - {{ tag }}
{% endfor %}
{% endfor %}

#
# Job execution configuration
#
execution:
  default: {{ galaxy_job_conf_default_environment | default('job_router') }}
  environments:

    #
    # dynamic environments
    #
    job_router:
      runner: dynamic
      function: job_router
{% for rule in galaxy_job_conf_extra_dynamic_rules %}
    {{ rule }}:
      runner: dynamic
      function: {{ rule }}
{% endfor %}

    #
    # roundup and TACC-discretionary Jetstream environments
    #
{% for env in galaxy_job_conf_environments.slurm_environments %}
    slurm_{{ env.id }}:
      runner: slurm
      native_specification: {{ env.native_spec }}
      {{ env_tag_params(env.tags | default(none)) }}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
{% if env.create_test_env | default(false) %}
    test_slurm_{{ env.id }}:
      runner: slurm
      native_specification: --partition=reserved,jsreserved --nodes=1 --ntasks=1 --time=00:30:00 --mem=3840
      tags:
        - test
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
{% endif %}
{% if env.create_reserved_env | default(false) %}
    reserved_slurm_{{ env.id }}:
      runner: slurm
      native_specification: {{ env.native_spec | replace('normal', 'reserved') | replace('multi', 'reserved') }}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
{% endif %}
{% if env.create_deps_env | default(false) %}
    # Mount Conda deps in Singularity
    slurm_{{ env.id }}_deps:
      runner: slurm
      native_specification: {{ env.native_spec }}
      {{ env_tag_params(env.tags | default(none)) }}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      container_override:
        - type: singularity
          shell: '/bin/sh'
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: why?
        - name: SINGULARITYENV_PREPEND_PATH
          value: $PATH
        {{ env_temp_envs() }}
{% if env.create_test_env | default(false) %}
    test_slurm_{{ env.id }}_deps:
      runner: slurm
      native_specification: --partition=reserved,jsreserved --nodes=1 --ntasks=1 --time=00:30:00 --mem=3840
      tags:
        - test
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      container_override:
        - type: singularity
          shell: '/bin/sh'
          resolve_dependencies: true
          identifier: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: why?
        - name: SINGULARITYENV_PREPEND_PATH
          value: $PATH
        {{ env_temp_envs() }}
{% endif %}
{% endif %}
{% if env.create_legacy_env | default(false) %}
{#
    # FIXME: should more accurately be called "py2 env"
    # the command will run in the py2 container and default to using the galaxy venv for set_meta so this is no longer
    # needed
    #slurm_{{ env.id }}_legacy:
    #  runner: slurm
    #  native_specification: {{ env.native_spec }}
    #  use_metadata_binary: true
    #  {{ env_tag_params(env.tags | default(none)) }}
    #  env:
    #    {{ env_local_envs(java_mem=env.java_mem) }}
    #    {{ env_legacy_envs() }}
#}
    slurm_{{ env.id }}_legacy:
      runner: slurm
      native_specification: {{ env.native_spec }}
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:2.7.16"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
    # FIXME: only normal version of this is currently used
    slurm_{{ env.id }}_galaxy_env:
      runner: slurm
      native_specification: {{ env.native_spec }}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      # FIXME: why ubuntu?
      #singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/ubuntu:20.04"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
        {{ env_temp_envs() }}
{% if env.create_test_env | default(false) %}
    test_slurm_{{ env.id }}_legacy:
      runner: slurm
      native_specification: --partition=reserved,jsreserved --nodes=1 --ntasks=1 --time=00:30:00 --mem=3840
      tags:
        - test
      singularity_enabled: true
      singularity_volumes: "{{ galaxy_job_conf_singularity_volumes | join(',') }}"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:2.7.16"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
    # FIXME: only normal version of this is currently used
    test_slurm_{{ env.id }}_galaxy_env:
      runner: slurm
      native_specification: --partition=reserved,jsreserved --nodes=1 --ntasks=1 --time=00:30:00 --mem=3840
      tags:
        - test
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes)) | join(',') }}"
      # FIXME: why ubuntu?
      #singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/ubuntu:20.04"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
        {{ env_temp_envs() }}
{% endif %}
{% endif %}
{% endfor %}

    # FIXME:
    slurm_upload:
      runner: slurm
      native_specification: --partition=normal,jsnormal --nodes=1 --ntasks=1 --time=12:00:00 --mem=3840
{#
      singularity_enabled: true
      singularity_volumes: "{{ (galaxy_job_conf_singularity_volumes + [galaxy_ftp_upload_dir ~ ':rw', galaxy_nginx_upload_dir ~ ':rw']) | join(',') }}"
      # FIXME: why ubuntu?
      #singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/ubuntu:20.04"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
        {{ env_temp_envs() }}
#}
      env:
        - name: TEMP
          #raw: true
          value: $(dirname ${BASH_SOURCE[0]})/_job_tmp
        - name: TMPDIR
          value: $TEMP
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$TEMP
        - execute: mkdir -p $TEMP
        - name: LC_ALL
          value: C

    # FIXME: roundup-only since I'm pretty certain memory_limit_reached doesn't work on Pulsar dests
    # resubmit to jetstream-tacc-xlarge on memory failure
    slurm_multi_memory_resubmit:
      runner: slurm
      native_specification: --partition=multi,jsmulti --nodes=1 --ntasks=6 --time=36:00:00
      {{ env_tag_params(['multi'] | default(none)) }}
      env:
        {{ env_local_envs(java_mem=28) }}
      resubmit:
        condition: memory_limit_reached
        environment: jetstream_tacc_xlarge

    #
    # Kubernetes environments
    #
    tacc_k8s:
      runner: tacc_k8s
      docker_enabled: true  # probably shouldn't be needed but is still
      #docker_default_container_id: 'quay.io/biocontainers/coreutils:8.31--h14c3975_0'
      #docker_default_container_id: 'python:3.6-buster'
      docker_default_container_id: 'quay.io/biocontainers/python:3.6.7'
      pulsar_container_image: 'quay.io/galaxy/pulsar-pod-staging:0.14.0'
      k8s_namespace: ndc
      #k8s_walltime_limit: 86400  # 24 hours
      k8s_walltime_limit: 43200  # 12 hours
      pulsar_requests_cpu: 0.5
      pulsar_requests_memory: 0.5Gi
      pulsar_limits_cpu: 0.5
      pulsar_limits_memory: 0.5Gi
      tool_requests_cpu: 1.5
      tool_requests_memory: 1.5Gi
      tool_limits_cpu: 1.5
      tool_limits_memory: 1.5Gi
      #jobs_directory: /not/a/real/path
      pulsar_app_config_path: {{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml
      # Specify a non-default Pulsar staging container.
      # Generate job names with a string unique to this Galaxy (see
      # Kubernetes runner description).
      #k8s_galaxy_instance_id: mycoolgalaxy
      # Path to Kubernetes configuration fil (see Kubernetes runner description.)
      #k8s_config_path: /path/to/kubeconfig

    #
    # Jetstream environments
    #
{% for env in galaxy_job_conf_environments.jetstream_environments %}
    jetstream_{{ env.id }}:
      runner: {{ env.runner }}
      submit_native_specification: {{ env.native_spec }}
      {{ env_pulsar_params() }}
      {{ env_jetstream_params() }}
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        - name: PATH
          value: /jetstream/scratch0/{{ galaxy_instance_codename }}/conda/envs/set_metadata@20171114/bin:$PATH
        {{ env_temp_envs() }}
    jetstream_{{ env.id }}_singularity:
      runner: {{ env.runner }}
      submit_native_specification: {{ env.native_spec }}
      {{ env_pulsar_params() }}
      {{ env_jetstream_params() }}
      {{ env_tag_params(env.tags | default(none)) }}
      singularity_enabled: true
      # /cvmfs/data?
      #singularity_volumes: '$galaxy_root:ro,$tool_directory:ro,$working_directory:rw,$job_directory:rw,/galaxy-repl:ro,/galaxy:ro'
      singularity_volumes: '$galaxy_root:ro,$tool_directory:ro,$working_directory:rw,$job_directory:rw,/galaxy-repl/test/object_store_cache:ro,/cvmfs/data.galaxyproject.org:ro'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
      env:
        {{ env_local_envs(java_mem=env.java_mem) }}
{% if env.create_reserved_env | default(false) %}
    reserved_jetstream_{{ env.id }}:
      runner: {{ env.runner }}
      native_specification: {{ env.native_spec | replace('normal', 'reserved') | replace('multi', 'reserved') }}
      {{ env_pulsar_params() }}
      {{ env_jetstream_params() }}
      env:
        {{ env_pulsar_envs() }}
        - name: PATH
          value: /jetstream/scratch0/{{ galaxy_instance_codename }}/conda/envs/set_metadata@20171114/bin:$PATH
        {{ env_temp_envs() }}
{% endif %}
{% if env.create_legacy_env | default(false) %}
    jetstream_{{ env.id }}_legacy:
      runner: {{ env.runner }}
      native_specification: {{ env.native_spec }}
      use_metadata_binary: true
      {{ env_pulsar_params() }}
      {{ env_jetstream_params() }}
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        {{ env_legacy_envs() }}
{% endif %}
{% endfor %}

    #
    # Stampede environments
    #
{% for env in galaxy_job_conf_environments.stampede_environments %}
    stampede_{{ env.id }}:
      runner: stampede
      submit_native_specification: {{ env.native_spec }}
      {{ env_pulsar_params() }}
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {# this doesn't work, set in supervisor environment instead
       #remote_property_galaxy_virtual_env: /work/galaxy/{{ galaxy_instance_codename }}/galaxy/venv
      -#}
      {# this used to work but doesn't now either, set in supervisor environment instead, however, the Pulsar client
       # still requires it to be set when remote_metadata is enabled -#}
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      file_action_config: {{ galaxy_config_dir }}/pulsar_stampede_actions.yml
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        - execute: eval `/opt/apps/lmod/lmod/libexec/lmod bash purge`
        {# Stampede assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        {# Mem=0 if the Slurm -mem param is not used, which is not allowed on Stampede2 -#}
        - name: GALAXY_MEMORY_MB
          value: "{{ env.memory_mb }}"
{% endfor %}

    #
    # Frontera environments
    #
{% for env in galaxy_job_conf_environments.frontera_environments %}
    frontera_{{ env.id }}:
      runner: frontera
      submit_native_specification: {{ env.native_spec }}
      {{ env_pulsar_params(remote_metadata='false') }}
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {{ env_tag_params(env.tags | default(none)) }}
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        {{ env_pulsar_envs() }}
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        {# Frontera assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        {# Mem=0 if the Slurm -mem param is not used, which is not allowed on Frontera -#}
        - name: GALAXY_MEMORY_MB
          value: "{{ env.memory_mb }}"
{% endfor %}

    #
    # Bridges environments
    #
{% for env in galaxy_job_conf_environments.bridges_environments %}
    bridges_{{ env.id }}:
      runner: bridges
      submit_native_specification: {{ env.native_spec }}
      {{ env_pulsar_params(remote_metadata='false') }}
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {# see stampede comments -#}
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      file_action_config: {{ galaxy_config_dir }}/pulsar_bridges_actions.yml
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        #- execute: eval `modulecmd sh purge`
        {# Bridges-2 assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
{% endfor %}

#
# Job resource selector configuration
#
resources:
  groups:
{% for group, elements in galaxy_job_conf_resource_groups.items() %}
    {{ group }}:
{% for element in elements %}
      - {{ element }}
{% endfor %}
{% endfor %}

#
# Job mapping configuration
#
tools:
  # Because job_router is the default environment, you should not need to define explicit mappings here unless:
  #   1. you want them to have a job resource param selector (but there are lists for that in tools_conf.yml
  #   2. you want them to have explicit handlers
  #   3. you don't want a tool to use the router

  # Explicit mappings
{% for tool in galaxy_job_conf_tools %}
  - {{ tool | to_yaml | trim }}
{% endfor %}

  # Bridges tools
{% for id in galaxy_large_memory_tools %}
  - id: {{ id }}
    handler: multi
    resources: bridges
{% endfor %}

{% for id in galaxy_conditional_large_memory_tools %}
  - id: {{ id }}
    handler: multi
    resources: multi_bridges
{% endfor %}

  # Stampede tools
{% for id in galaxy_low_priority_tools %}
  - id: {{ id }}
    handler: multi
    resources: stampede
{% endfor %}

  # Frontera tools
{% for id in galaxy_frontera_tools %}
  - id: {{ id }}
    handler: multi
    resources: frontera
{% endfor %}

  # Long walltime tools
  - id: fasterq_dump
    handler: multi
    resources: multi

  # Specially mapped (no resource selector) multicore tools
  - id: rna_star
    handler: multi
  - id: rna_starsolo
    handler: multi

  # This is an even longer walltime tool overridden in the job router spec
  - id: align_families
    handler: multi

  # Standard multicore tools
{% for id in galaxy_multicore_tools %}
  - id: {{ id }}
    handler: multi
    resources: multi
{% endfor %}

#
# Job limits configuration
#
limits:
{% for limit in galaxy_job_conf_limits %}
  - {{ limit | to_yaml | trim }}
{% endfor %}
