---
##
## This file is maintained by Ansible - CHANGES WILL BE OVERWRITTEN
##

{#
## WARNING: this file is shared between Test and Main!
##
## NOTE: many template values can be found in env/<env>/group_vars/galaxyservers/tools_conf.yml
-#}

{#-
## template macros
#}

{%- macro runner_pulsar_params() -%}
    amqp_url: {{ galaxy_job_conf_amqp_url }}
    galaxy_url: {{ galaxy_job_conf_pulsar_galaxy_url }}
    persistence_directory: /srv/galaxy/{{ galaxy_instance_codename }}/var/pulsar_amqp_ack
    amqp_acknowledge: true
    amqp_ack_republish_time: 1200
    amqp_consumer_timeout: 2.0
    amqp_publish_retry: true
    amqp_publish_retry_max_retries: 60
{%- endmacro %}

{%- macro env_local_envs(java_mem=7, singularity=true) -%}
{#-
        # TODO: what uses this (DejaVu fonts)?
-#}
        - name: XDG_DATA_HOME
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/xdg/data
        {{ env_temp_envs(singularity=singularity) }}
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx{{ java_mem }}g -Xms256m
        - execute: ulimit -c 0
        - name: LC_ALL
          value: C
{% if singularity %}
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
{% else %}
        - name: PATH
          value: /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
{% endif %}
{%- endmacro %}

{%- macro env_pulsar_envs(java_mem=7, singularity=true) -%}
        - name: _JAVA_OPTIONS
          value: $_JAVA_OPTIONS -Xmx{{ java_mem }}g -Xms256m
        - name: LC_ALL
          value: C
        - name: TERM
          value: vt100
        - execute: ulimit -c 0
{% if singularity %}
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
        - name: SINGULARITYENV_LC_ALL
          value: $LC_ALL
        - name: SINGULARITYENV_TERM
          value: $TERM
{% endif %}
{%- endmacro %}

{%- macro env_temp_envs(singularity=true) -%}
        - name: TEMP
          #raw: true
          value: $(dirname ${BASH_SOURCE[0]})/_job_tmp
        - name: TMPDIR
          value: $TEMP
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$TEMP
        - execute: mkdir -p $TEMP
{% if singularity %}
        - name: SINGULARITYENV_TEMP
          value: $TEMP
        - name: SINGULARITYENV_TMPDIR
          value: $TEMP
        - name: SINGULARITYENV__JAVA_OPTIONS
          value: $_JAVA_OPTIONS
{% endif %}
{%- endmacro %}

{%- macro env_tag_params(tags) -%}
{% if tags is not none -%}
      tags: {{ tags }}
{%- endif -%}
{%- endmacro %}

{%- macro env_pulsar_params(remote_metadata='true', dependency_resolution='remote') -%}
      remote_metadata: {{ remote_metadata }}
      transport: curl
      default_file_action: remote_transfer
      dependency_resolution: {{ dependency_resolution }}
      rewrite_parameters: true
      submit_user_email: $__user_email__
      outputs_to_working_directory: false
{%- endmacro %}

{%- macro env_jetstream_params() -%}
      jobs_directory: /jetstream/scratch0/{{ galaxy_instance_codename }}/jobs
{#
      # this doesn't work, set in supervisor environment instead
      #remote_property_galaxy_virtual_env: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv
#}
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      file_action_config: {{ galaxy_config_dir }}/pulsar_jetstream_actions.yml
{%- endmacro %}

{%- macro env_jetstream2_params() -%}
      jobs_directory: /jetstream2/scratch/{{ galaxy_instance_codename }}/jobs
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
{%- endmacro %}

#
# Job runner plugin configuration
#
runners:
  dynamic:
    # these live in the virtualenv
    rules_module: usegalaxy.jobs.rules
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 2
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib64/libdrmaa.so
    invalidjobexception_retries: 5
    internalexception_retries: 5
  jetstream_iu:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream_iu
    {{ runner_pulsar_params() }}
  jetstream_tacc:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream_tacc
    {{ runner_pulsar_params() }}
  jetstream2:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream2
    {{ runner_pulsar_params() }}
  stampede:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: stampede
    {{ runner_pulsar_params() }}
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
    {{ runner_pulsar_params() }}
  bridges:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: bridges
    {{ runner_pulsar_params() }}
  tacc_k8s:
    load: galaxy.jobs.runners.pulsar:PulsarKubernetesJobRunner
    manager: tacc_k8s
    {{ runner_pulsar_params() }}

#
# Job handler configuration
#
handling:
  #default: handlers
  assign:
    - db-skip-locked
  max_grab: 16
  ready_window_size: 32
  processes:
{% for handler in galaxy_job_conf_handlers %}
    {{ handler.id }}:
      plugins:
{% for plugin in handler.plugins %}
        - {{ plugin }}
{% endfor %}
{% if handler.tags is defined and handler.tags %}
      tags:
{% for tag in handler.tags %}
        - {{ tag }}
{% endfor %}
{% endif %}
{% endfor %}

#
# Job execution configuration
#
execution:
  default: {{ galaxy_job_conf_default_environment | default('job_router') }}
  environments:

    #
    # dynamic environments
    #
    job_router:
      runner: dynamic
      function: job_router
{% for rule in galaxy_job_conf_extra_dynamic_rules %}
    {{ rule }}:
      runner: dynamic
      function: {{ rule }}
{% endfor %}

    #
    # roundup and TACC-discretionary Jetstream environments
    #
{% for env in galaxy_job_conf_environments.slurm_environments %}
{% set use_singularity = env.use_singularity | default(true) %}
{% set java_mem = env.java_mem | default(7) %}
    slurm_{{ env.id }}:
      runner: slurm
      native_specification: {{ env.native_specification }}
      {{ env_tag_params(env.tags | default(none)) }}
{% if use_singularity %}
      outputs_to_working_directory: true
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes.local)) | join(',') }}"
{% if 'container_override' in env %}
      container_override: {{ env.container_override | to_yaml(width=9999, default_flow_style=True) }}
{% else %}
      singularity_default_container_id: "{{ env.default_container_id | default(galaxy_job_conf_default_container_id) }}"
{% endif %}
{% endif %}
{% for env_param_name, env_param_value in (env.extra_params | default({})).items() %}
      {{ env_param_name }}: {{ env_param_value }}
{% endfor %}
      env:
        {{ env_local_envs(java_mem=java_mem, singularity=use_singularity) }}
{% for env_env in (env.append_envs | default([])) %}
        - {{ env_env }}
{% endfor %}
{% endfor %}

{#
    # FIXME:
    slurm_upload:
      runner: slurm
      native_specification: --partition=normal,jsnormal --nodes=1 --ntasks=1 --time=12:00:00 --mem=3840
      singularity_enabled: true
      singularity_volumes: "{{ (galaxy_job_conf_singularity_volumes.local + [galaxy_ftp_upload_dir ~ ':rw', galaxy_nginx_upload_dir ~ ':rw']) | join(',') }}"
      # FIXME: why ubuntu?
      #singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/ubuntu:20.04"
      singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/centos:8.3.2011"
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        - name: SINGULARITYENV_LC_ALL
          value: C
        # FIXME: GALAXY_VIRTUAL_ENV/VIRTUAL_ENV not needed?
        - name: SINGULARITYENV_PREPEND_PATH
          value: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/venv/bin
        {{ env_temp_envs() }}
      env:
        - name: TEMP
          #raw: true
          value: $(dirname ${BASH_SOURCE[0]})/_job_tmp
        - name: TMPDIR
          value: $TEMP
        - name: _JAVA_OPTIONS
          value: -Djava.io.tmpdir=$TEMP
        - execute: mkdir -p $TEMP
        - name: LC_ALL
          value: C
#}

    # FIXME: roundup-only since I'm pretty certain memory_limit_reached doesn't work on Pulsar dests
    # resubmit to jetstream-tacc-xlarge on memory failure
    slurm_multi_memory_resubmit:
      runner: slurm
      native_specification: --partition=multi,jsmulti --nodes=1 --ntasks=6 --time=36:00:00
      {{ env_tag_params(['multi'] | default(none)) }}
      env:
        {{ env_local_envs(java_mem=28) }}
      resubmit:
        condition: memory_limit_reached
        environment: jetstream_tacc_xlarge

    #
    # Kubernetes environments
    #
    tacc_k8s:
      runner: tacc_k8s
      docker_enabled: true  # probably shouldn't be needed but is still
      outputs_to_working_directory: false
      container_resolvers:
        - type: explicit
        #- type: cached_mulled
      #docker_default_container_id: 'quay.io/biocontainers/coreutils:8.31--h14c3975_0'
      #docker_default_container_id: 'python:3.6-buster'
      docker_default_container_id: 'quay.io/biocontainers/python:3.6.7'
      pulsar_container_image: '{{ pulsar_coexecution_container_image | default("quay.io/galaxy/pulsar-pod-staging:0.14.0") }}'
      k8s_namespace: ndc
      #k8s_walltime_limit: 86400  # 24 hours
      k8s_walltime_limit: 43200  # 12 hours
      pulsar_requests_cpu: 0.5
      pulsar_requests_memory: 0.5Gi
      pulsar_limits_cpu: 0.5
      pulsar_limits_memory: 0.5Gi
      tool_requests_cpu: 1.5
      tool_requests_memory: 1.5Gi
      tool_limits_cpu: 1.5
      tool_limits_memory: 1.5Gi
      #jobs_directory: /not/a/real/path
      pulsar_app_config_path: {{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml
      # Specify a non-default Pulsar staging container.
      # Generate job names with a string unique to this Galaxy (see
      # Kubernetes runner description).
      #k8s_galaxy_instance_id: mycoolgalaxy
      # Path to Kubernetes configuration fil (see Kubernetes runner description.)
      #k8s_config_path: /path/to/kubeconfig

    #
    # Jetstream environments
    #
{% for env in galaxy_job_conf_environments.jetstream_environments %}
{% set use_singularity = env.use_singularity | default(true) %}
{% set java_mem = env.java_mem | default(7) %}
    jetstream_{{ env.id }}:
      runner: {{ env.runner }}
      submit_native_specification: {{ env.native_specification }}
      {{ env_tag_params(env.tags | default(none)) }}
      {{ env_pulsar_params(remote_metadata=(not use_singularity)) }}
      {{ env_jetstream_params() }}
{% if use_singularity %}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes.jetstream)) | join(',') }}"
{% if 'container_override' in env %}
      container_override: {{ env.container_override }}
{% else %}
      singularity_default_container_id: "{{ env.default_container_id | default(galaxy_job_conf_default_container_id) }}"
{% endif %}
{% endif %}
{% for env_param_name, env_param_value in (env.extra_params | default({})).items() %}
      {{ env_param_name }}: {{ env_param_value }}
{% endfor %}
      env:
        {{ env_temp_envs(singularity=use_singularity) }}
        {{ env_pulsar_envs(java_mem=java_mem, singularity=use_singularity) }}
{% for env_env in (env.append_envs | default([])) %}
        - {{ env_env }}
{% endfor %}
{% if not use_singularity %}
        - name: PATH
          value: /jetstream/scratch0/{{ galaxy_instance_codename }}/conda/envs/set_metadata@20171114/bin:$PATH
{% endif %}
{% endfor %}

    #
    # Jetstream2 environments
    #
{% for env in galaxy_job_conf_environments.jetstream2_environments %}
{% set use_singularity = env.use_singularity | default(true) %}
{% set java_mem = env.java_mem | default(5) %}
    jetstream2_{{ env.id }}:
      runner: jetstream2
      submit_native_specification: {{ env.native_specification }}
      {{ env_tag_params(env.tags | default(none)) }}
      {{ env_pulsar_params(remote_metadata=(not use_singularity)) }}
      {{ env_jetstream2_params() }}
{% if use_singularity %}
      singularity_enabled: true
      singularity_volumes: "{{ (env.volumes | default(galaxy_job_conf_singularity_volumes.jetstream)) | join(',') }}"
{% if 'container_override' in env %}
      container_override: {{ env.container_override }}
{% else %}
      singularity_default_container_id: "{{ env.default_container_id | default(galaxy_job_conf_default_container_id) }}"
{% endif %}
{% endif %}
{% for env_param_name, env_param_value in (env.extra_params | default({})).items() %}
      {{ env_param_name }}: {{ env_param_value }}
{% endfor %}
      env:
        {{ env_temp_envs(singularity=use_singularity) }}
        {{ env_pulsar_envs(java_mem=java_mem, singularity=use_singularity) }}
{% for env_env in (env.append_envs | default([])) %}
        - {{ env_env }}
{% endfor %}
{% endfor %}

    #
    # Stampede environments
    #
{% for env in galaxy_job_conf_environments.stampede_environments %}
    stampede_{{ env.id }}:
      runner: stampede
      submit_native_specification: {{ env.native_specification }}
      {{ env_pulsar_params() }}
      jobs_directory: /scratch/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {# this doesn't work, set in supervisor environment instead
       #remote_property_galaxy_virtual_env: /work/galaxy/{{ galaxy_instance_codename }}/galaxy/venv
      -#}
      {# this used to work but doesn't now either, set in supervisor environment instead, however, the Pulsar client
       # still requires it to be set when remote_metadata is enabled -#}
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      file_action_config: {{ galaxy_config_dir }}/pulsar_stampede_actions.yml
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        - execute: eval `/opt/apps/lmod/lmod/libexec/lmod bash purge`
        {# Stampede assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        {# Mem=0 if the Slurm -mem param is not used, which is not allowed on Stampede2 -#}
        - name: GALAXY_MEMORY_MB
          value: "{{ env.memory_mb }}"
{% endfor %}

    #
    # Frontera environments
    #
{% for env in galaxy_job_conf_environments.frontera_environments %}
    frontera_{{ env.id }}:
      runner: frontera
      submit_native_specification: {{ env.native_specification }}
      {{ env_pulsar_params(remote_metadata='false') }}
      jobs_directory: /scratch1/03166/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {{ env_tag_params(env.tags | default(none)) }}
      singularity_enabled: true
      # the tacc-singularity module automatically sets up mounts
      singularity_volumes: null
      container_resolvers:
        - type: explicit_singularity
        - type: mulled_singularity
      require_container: true
      env:
        {{ env_pulsar_envs() }}
        - file: /etc/profile.d/z01_lmod.sh
        - execute: module load tacc-singularity
        {# Frontera assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        {# Mem=0 if the Slurm -mem param is not used, which is not allowed on Frontera -#}
        - name: GALAXY_MEMORY_MB
          value: "{{ env.memory_mb }}"
{% endfor %}

    #
    # Bridges environments
    #
{% for env in galaxy_job_conf_environments.bridges_environments %}
    bridges_{{ env.id }}:
      runner: bridges
      submit_native_specification: {{ env.native_specification }}
      {{ env_pulsar_params(remote_metadata='false') }}
      jobs_directory: /ocean/projects/mcb140028p/xcgalaxy/{{ galaxy_instance_codename }}/staging/
      {# see stampede comments -#}
      remote_property_galaxy_home: /cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org/galaxy
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        #- execute: eval `modulecmd sh purge`
        {# Bridges-2 assigns whole nodes, so $SLURM_CPUS_ON_NODE is not the same as the requested number of tasks -#}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: TRINITY_SCRATCH_DIR
          value: "$LOCAL"
{% endfor %}

    #
    # Expanse environments
    #
{% for env in galaxy_job_conf_environments.expanse_environments %}
    expanse_{{ env.id }}:
      runner: expanse
      submit_native_specification: {{ env.native_specification }}
      {{ env_pulsar_params(remote_metadata='false') }}
      jobs_directory: /expanse/lustre/scratch/xgalaxy/temp_project/{{ galaxy_instance_codename }}/staging/
      {{ env_tag_params(env.tags | default(none)) }}
      env:
        {{ env_pulsar_envs() }}
        - name: GALAXY_SLOTS
          value: "$SLURM_NTASKS"
        - name: TRINITY_SCRATCH_DIR
          value: "/scratch/xgalaxy/job_$SLURM_JOB_ID"
{% endfor %}

#
# Job resource selector configuration
#
resources:
  groups:
{% for group, elements in galaxy_job_conf_resource_groups.items() %}
    {{ group }}:
{% for element in elements %}
      - {{ element }}
{% endfor %}
{% endfor %}

#
# Job mapping configuration
#
tools:
  # Because job_router is the default environment, you should not need to define explicit mappings here unless:
  #   1. you want them to have a job resource param selector (but there are lists for that in tools_conf.yml
  #   2. you want them to have explicit handlers
  #   3. you don't want a tool to use the router

  # Explicit mappings
{% for tool in galaxy_job_conf_tools %}
  - {{ tool | to_yaml | trim }}
{% endfor %}

  # Bridges tools
{% for id in galaxy_large_memory_tools %}
  - id: {{ id }}
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: bridges
{% endfor %}

{% for id in galaxy_conditional_large_memory_tools %}
  - id: {{ id }}
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: multi_bridges
{% endfor %}

  # Stampede tools
{% for id in galaxy_low_priority_tools %}
  - id: {{ id }}
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: stampede
{% endfor %}

  # Frontera tools
{% for id in galaxy_frontera_tools %}
  - id: {{ id }}
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: frontera
{% endfor %}

  # Long walltime tools
  - id: fasterq_dump
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: multi

  # Specially mapped (no resource selector) multicore tools
  - id: rna_star
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
  - id: rna_starsolo
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}

  # This is an even longer walltime tool overridden in the job router spec
  - id: align_families
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}

  # Standard multicore tools
{% for id in galaxy_multicore_tools %}
  - id: {{ id }}
{% if galaxy_job_conf_multi_handler_tag is defined %}
    handler: {{ galaxy_job_conf_multi_handler_tag }}
{% endif %}
    resources: multi
{% endfor %}

#
# Job limits configuration
#
limits:
{% for limit in galaxy_job_conf_limits %}
  - {{ limit | to_yaml | trim }}
{% endfor %}
