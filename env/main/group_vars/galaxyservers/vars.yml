---

# FIXME:
#   - The proxy was cloned by hand in /srv/galaxy/main/proxy/gx-it-proxy
#   - venv w/ nodeenv and node 10.15.3 installed by hand in /srv/galaxy/main/proxy/venv
#   - npm install run by hand (must activate venv or otherwise have venv's node on path)
#   - NOTE: node 12+ does not work with the pinned sqlite3 package.
#   - gxit_proxy_port is hardcoded in the nginx template since that template is deployed from the test env
gxit_proxy_port: 8980
gxit_proxy_forward_ip: 172.16.120.50
gxit_proxy_forward_port: 31946
gxit_proxy_nodeenv: /srv/galaxy/main/proxy/venv

## these vars are defined in vault.yml
#
# used by: galaxyproject.galaxy (templating job_conf.yml)
galaxy_job_conf_amqp_url: "{{ vault_galaxy_job_conf_amqp_url }}"

# used by: object store config template
galaxy_icat_irods_password: "{{ vault_galaxy_icat_irods_password }}"

# file_sources_conf.yml
covid_crg_ftp_staging_user: "{{ vault_covid_crg_ftp_staging_user }}"
covid_crg_ftp_staging_passwd: "{{ vault_covid_crg_ftp_staging_passwd }}"
genomeark_galaxy_aws_secret_access_key: "{{ vault_genomeark_galaxy_aws_secret_access_key }}"
genomeark_galaxy_aws_access_key_id: "{{ vault_genomeark_galaxy_aws_access_key_id }}"

## used by: galaxyproject.galaxy and the play itself
galaxy_system_group: G-803372

# FIXME: "{{ galaxy_remote_users.privsep | default(omit) }}" is blanking remote_user on the play. I set the below option
# to workaround but this needs to be fixed in galaxyproject.galaxy, nothing should be breaking when privsep mode is not
# enabled in the role
galaxy_remote_users:
  privsep: "{{ galaxy_privileged_user }}"
  errdocs: "{{ galaxy_privileged_user }}"
  galaxy: "{{ galaxy_user }}"
  root: root

# FIXME: Same thing, the become log checks if galaxy_become_users.privsep is set and yet in the defaults we cause it to
# be always set
galaxy_become_users: {}

galaxy_server_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
galaxy_shed_tools_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/shed_tools
galaxy_shed_tool_conf_file: /cvmfs/{{ galaxy_cvmfs_repo }}/config/shed_tool_conf.xml

#galaxy_venv_dir defined in all.yml
galaxy_config_dir: "{{ galaxy_root }}/config"
galaxy_config_file: "{{ galaxy_config_dir }}/galaxy.yml"
galaxy_mutable_data_dir: "{{ galaxy_root }}/var"
galaxy_mutable_config_dir: "/corral4/{{ galaxy_instance_codename }}/config"
# these don't need to be set if the galaxyproject.galaxy layout tasks run, but usegalaxy_backup uses
# galaxyproject.galaxy's defaults without running the layout tasks and breaks if they are unset
galaxy_cache_dir: "{{ galaxy_mutable_data_dir }}/cache"

galaxy_admin_email_to: galaxy-lab@bx.psu.edu


## used by: job_conf.yml template
galaxy_job_conf_pulsar_galaxy_url: "https://{{ inventory_hostname_short }}.galaxyproject.org"

pulsar_coexecution_container_image: quay.io/natefoo/pulsar-pod-staging:0.15.0.dev0

__handler_default_plugins:
  - slurm
  - jetstream2

__handler_multi_plugins:
  - slurm
  - jetstream_iu
  - jetstream_tacc
  - jetstream2
  - stampede
  - frontera
  - bridges
  - expanse

galaxy_systemd_memory_limit:
  # FIXME: this shouldn't be mule anymore. also "G" is hardcoded, but % would be nice.
  mule: 12

# FIXME: remove this and its handling in the job conf template if you do end up making all handlers load all plugins
galaxy_job_conf_multi_handler_tag: multi

# galaxy_job_conf_handlers is in host_vars

# NOTE: do not include job_router, it is included by default
galaxy_job_conf_extra_dynamic_rules:
  - dynamic_nvc_dynamic_memory
  # TODO: remove all below once everything runs through the job router
  #- dynamic_normal_reserved
  #- dynamic_normal_16gb_reserved
  #- dynamic_normal_32gb_reserved
  #- dynamic_normal_64gb_reserved
  #- dynamic_multi_reserved
  #- dynamic_multi_long_reserved
  #- dynamic_local_stampede_select_dynamic_walltime
  #- dynamic_multi_bridges_select
  #- dynamic_stampede_select
  #- dynamic_bridges_select
  #- dynamic_rnastar

#galaxy_job_conf_default_environment: dynamic_normal_reserved
galaxy_job_conf_default_environment: job_router

galaxy_job_conf_default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:3.8.3

galaxy_job_conf_singularity_volumes:
  local:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "{{ galaxy_object_store_cache_path }}:ro"
    - "{{ galaxy_new_file_path }}:ro"
    - "/corral4/{{ galaxy_instance_codename }}/objects:ro"
    - "/corral4/{{ galaxy_instance_codename }}/files:ro"
    - "/corral4/{{ galaxy_instance_codename }}/psufiles:ro"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  jetstream:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"

galaxy_job_conf_resource_groups:
  multi: [multi_compute_resource]
  multi_long: [multi_long_compute_resource]
  multi_bridges: [multi_bridges_compute_resource]
  multi_frontera: [multi_frontera_compute_resource, ntasks, time]
  stampede: [stampede_compute_resource]
  frontera: [frontera_compute_resource, ntasks, time]
  bridges: [bridges_compute_resource]

# tool mappings are in tools_conf.yml

galaxy_job_conf_limits:
  # this is a failsafe more than anything - actual limits are enforced on the environments
  - type: registered_user_concurrent_jobs
    value: 32
  - type: anonymous_user_concurrent_jobs
    value: 1

  # these probably don't really work
  - type: walltime
    value: '194:00:00'
  - type: output_size
    value: 200G

  # per-environments per-user limits

  # env tags (groups) for limits
  - type: environment_user_concurrent_jobs
    id: training_limit
    value: 2
  - type: environment_user_concurrent_jobs
    id: normal_limit
    value: 8
  - type: environment_user_concurrent_jobs
    id: multi_limit
    value: 4
  - type: environment_user_concurrent_jobs
    id: multi_long_limit
    value: 1
  - type: environment_user_concurrent_jobs
    id: test_limit
    value: 8
  - type: environment_user_concurrent_jobs
    id: multi_largemem_limit
    value: 1
  - type: environment_user_concurrent_jobs
    id: gpu_limit
    value: 1

  # granular limits for singularity envs
  - type: environment_user_concurrent_jobs
    id: slurm_normal
    value: 6
  - type: environment_user_concurrent_jobs
    id: normal
    value: 6
  - type: environment_user_concurrent_jobs
    id: slurm_normal_16gb
    value: 2
  - type: environment_user_concurrent_jobs
    id: slurm_normal_32gb
    value: 1
  - type: environment_user_concurrent_jobs
    id: slurm_normal_64gb
    value: 1
  - type: environment_user_concurrent_jobs
    id: slurm_multi_development
    value: 1

  # pulsar envs
  - type: environment_user_concurrent_jobs
    id: stampede_normal
    value: 4
  - type: environment_user_concurrent_jobs
    id: stampede_skx_normal
    value: 4
  - type: environment_user_concurrent_jobs
    id: stampede_development
    value: 1
  - type: environment_user_concurrent_jobs
    id: stampede_skx_development
    value: 1
  - type: environment_user_concurrent_jobs
    id: frontera_small
    value: 2
  - type: environment_user_concurrent_jobs
    id: frontera_development
    value: 1
  - type: environment_user_concurrent_jobs
    id: bridges_normal
    value: 2
  - type: environment_user_concurrent_jobs
    id: bridges_shared_128gb
    value: 2
  - type: environment_user_concurrent_jobs
    id: bridges_shared_64gb
    value: 4
  - type: environment_user_concurrent_jobs
    id: bridges_extreme_1tb
    value: 1
  - type: environment_user_concurrent_jobs
    id: bridges_development
    value: 1
  - type: environment_user_concurrent_jobs
    id: expanse_normal
    value: 4
  - type: environment_user_concurrent_jobs
    id: expanse_shared_128gb
    value: 4
  - type: environment_user_concurrent_jobs
    id: expanse_shared_64gb
    value: 8
  - type: environment_user_concurrent_jobs
    id: expanse_development
    value: 2
  - type: environment_user_concurrent_jobs
    id: tacc_k8s
    value: 1

  # per-environments total limits
  - type: environment_total_concurrent_jobs
    id: stampede_normal
    value: 50
  - type: environment_total_concurrent_jobs
    id: stampede_skx_normal
    value: 50
  - type: environment_total_concurrent_jobs
    id: stampede_long
    value: 1
  - type: environment_total_concurrent_jobs
    id: stampede_development
    value: 1
  - type: environment_total_concurrent_jobs
    id: stampede_skx_development
    value: 1
  - type: environment_total_concurrent_jobs
    id: frontera_small
    value: 18
  - type: environment_total_concurrent_jobs
    id: frontera_development
    value: 1
  - type: environment_total_concurrent_jobs
    id: bridges_normal
    value: 100
  - type: environment_total_concurrent_jobs
    id: bridges_shared_64gb
    value: 50
  - type: environment_total_concurrent_jobs
    id: bridges_shared_128gb
    value: 50
  - type: environment_total_concurrent_jobs
    id: bridges_extreme_1tb
    value: 20
  - type: environment_total_concurrent_jobs
    id: bridges_development
    value: 4
  - type: environment_total_concurrent_jobs
    id: expanse_normal
    value: 100
  - type: environment_total_concurrent_jobs
    id: expanse_shared_64gb
    value: 50
  - type: environment_total_concurrent_jobs
    id: expanse_shared_128gb
    value: 50
  - type: environment_total_concurrent_jobs
    id: expanse_extreme_1tb
    value: 20
  - type: environment_total_concurrent_jobs
    id: expanse_development
    value: 4


## used by: galaxyproject.galaxy
galaxy_errordocs_dest: "{{ nginx_srv }}/{{ galaxy_instance_hostname }}/error"
galaxy_errordocs_502_message: |
  You are seeing this message because a request to Galaxy timed out or was refused. This may be a temporary issue which
  could be resolved by retrying the operation you were performing. If you receive this message repeatedly or for an
  extended amount of time, please check for additional information on the
  <a href="https://status.galaxyproject.org/">Galaxy status page</a> or the
  <a href="https://twitter.com/galaxyproject">@galaxyproject Twitter feed</a>. If the issue is not addressed on those
  sources, you may report it to the support team at
  <a href='mailto:galaxy-bugs@galaxyproject.org'>galaxy-bugs@galaxyproject.org</a>
  with details on what you were trying to do and the URL in the address bar.


## used by usegalaxy_privileged and templating job_conf.yml
galaxy_dynamic_rule_dir: "{{ galaxy_root }}/dynamic_rules"

galaxy_pgcleanup_actions:
  - delete_userless_histories
  - delete_exported_histories
  #- purge_deleted_users
  # this is less for GDPR purposes and more for allowing users to reregister with the same account later
  - purge_deleted_users_gdpr
  - purge_deleted_histories
  - purge_deleted_hdas
  - purge_historyless_hdas
  - purge_error_hdas
  - purge_hdas_of_purged_histories
  - delete_datasets
  - purge_datasets

## used by: usegalaxy_admin
galaxy_log_archive_dir: /corral4/{{ galaxy_instance_codename }}/backup/log


# galaxy_config hash moved to group_vars/all/galaxy_config_vars.yml

# specifies config files to copy from the playbook
galaxy_config_files:
  - src: files/galaxy/config/tool_data_table_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: files/galaxy/config/tool_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_conf.xml"
  - src: files/galaxy/config/data_manager_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['data_manager_config_file'] }}"
  - src: files/galaxy/config/tool_sheds_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['tool_sheds_config_file'] }}"
  - src: files/galaxy/config/job_metrics_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_metrics_config_file'] }}"
  - src: files/galaxy/config/job_resource_params_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_resource_params_file'] }}"
  - src: files/galaxy/config/dependency_resolvers_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['dependency_resolvers_config_file'] }}"
  - src: files/galaxy/config/pulsar_stampede_actions.yml
    dest: "{{ galaxy_config_dir }}/pulsar_stampede_actions.yml"
  - src: files/galaxy/config/pulsar_jetstream.yml
    dest: "{{ galaxy_config_dir }}/pulsar_jetstream.yml"
  - src: files/galaxy/config/pulsar_jetstream_actions.yml
    dest: "{{ galaxy_config_dir }}/pulsar_jetstream_actions.yml"
  - src: files/galaxy/config/disposable_email_blacklist.conf
    dest: "{{ galaxy_config[galaxy_app_config_section]['blacklist_file'] }}"
  - src: files/galaxy/config/workflow_schedulers_conf.xml
    dest: "{{ galaxy_config_dir }}/workflow_schedulers_conf.xml"
  - src: files/galaxy/config/user_preferences.yml
    dest: "{{ galaxy_config[galaxy_app_config_section]['user_preferences_extra_conf_path'] }}"

# specifies config files to template from the playbook
galaxy_config_templates:
  - src: templates/galaxy/config/job_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: templates/galaxy/config/build_sites.yml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['build_sites_config_file'] }}"
  - src: templates/galaxy/config/object_store_conf.xml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['object_store_config_file'] }}"
  - src: templates/galaxy/config/job_router_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/job_router_conf.yml"
  - src: templates/galaxy/config/tacc_k8s_pulsar_app_config.yml.j2
    dest: "{{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml"
  - src: templates/galaxy/config/file_sources_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/file_sources_conf.yml"
