---

## these vars are defined in vault.yml
#
# used by: galaxyproject.galaxy (templating job_conf.yml)
galaxy_job_conf_amqp_url: "{{ vault_galaxy_job_conf_amqp_url }}"

# used by: object store config template
galaxy_icat_irods_password: "{{ vault_galaxy_icat_irods_password }}"
galaxy_minio_idc_access_key: "idc"
galaxy_minio_idc_secret_key: "{{ vault_galaxy_minio_idc_secret_key }}"

# file_sources_conf.yml
covid_crg_ftp_staging_user: "{{ vault_covid_crg_ftp_staging_user }}"
covid_crg_ftp_staging_passwd: "{{ vault_covid_crg_ftp_staging_passwd }}"
genomeark_galaxy_aws_secret_access_key: "{{ vault_genomeark_galaxy_aws_secret_access_key }}"
genomeark_galaxy_aws_access_key_id: "{{ vault_genomeark_galaxy_aws_access_key_id }}"
genomeark_vgl_aws_secret_access_key: "{{ vault_genomeark_vgl_aws_secret_access_key }}"
genomeark_vgl_aws_access_key_id: "{{ vault_genomeark_vgl_aws_access_key_id }}"
paratus_aws_secret_access_key: "{{ vault_paratus_aws_secret_access_key }}"
paratus_aws_access_key_id: "{{ vault_paratus_aws_access_key_id }}"
elementbio_aws_secret_access_key: "{{ vault_elementbio_aws_secret_access_key }}"
elementbio_aws_access_key_id: "{{ vault_elementbio_aws_access_key_id }}"

## used by: galaxyproject.galaxy and the play itself
galaxy_system_group: G-803372

# FIXME: "{{ galaxy_remote_users.privsep | default(omit) }}" is blanking remote_user on the play. I set the below option
# to workaround but this needs to be fixed in galaxyproject.galaxy, nothing should be breaking when privsep mode is not
# enabled in the role
galaxy_remote_users:
  privsep: "{{ galaxy_privileged_user }}"
  errdocs: "{{ galaxy_privileged_user }}"
  galaxy: "{{ galaxy_user }}"
  root: root

# FIXME: Same thing, the become log checks if galaxy_become_users.privsep is set and yet in the defaults we cause it to
# be always set
galaxy_become_users: {}

galaxy_server_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/galaxy
galaxy_shed_tools_dir: /cvmfs/{{ galaxy_cvmfs_repo }}/shed_tools
galaxy_shed_tool_conf_file: /cvmfs/{{ galaxy_cvmfs_repo }}/config/shed_tool_conf.xml

#galaxy_venv_dir defined in all.yml
galaxy_config_dir: "{{ galaxy_root }}/config"
galaxy_config_file: "{{ galaxy_config_dir }}/galaxy.yml"
galaxy_mutable_data_dir: "{{ galaxy_root }}/var"
galaxy_mutable_config_dir: "/corral4/{{ galaxy_instance_codename }}/config"
# these don't need to be set if the galaxyproject.galaxy layout tasks run, but usegalaxy_backup uses
# galaxyproject.galaxy's defaults without running the layout tasks and breaks if they are unset
galaxy_cache_dir: "{{ galaxy_mutable_data_dir }}/cache"

galaxy_admin_email_to: galaxy-lab@bx.psu.edu


## used by: job_conf.yml template
galaxy_job_conf_pulsar_galaxy_url: "https://{{ inventory_hostname_short }}.galaxyproject.org"
galaxy_job_conf_persistence_directory: "/corral4/{{ galaxy_instance_codename }}/pulsar_amqp_ack"

galaxy_systemd_memory_limit:
  # FIXME: this shouldn't be mule anymore. also "G" is hardcoded, but % would be nice.
  mule: 12

# These are automatically added to any runners that load galaxy.jobs.runners.pulsar:...
galaxy_job_conf_pulsar_runner_params:
  amqp_url: "{{ galaxy_job_conf_amqp_url }}"
  galaxy_url: "{{ galaxy_job_conf_pulsar_galaxy_url }}"
  persistence_directory: "{{ galaxy_job_conf_persistence_directory }}"
  amqp_acknowledge: "true"
  amqp_ack_republish_time: "1200"
  amqp_consumer_timeout: "2.0"
  amqp_publish_retry: "true"
  amqp_publish_retry_max_retries: "60"

galaxy_job_conf_runners:
  slurm:
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
    workers: 4
    drmaa_library_path: /usr/lib64/libdrmaa.so
    invalidjobexception_retries: 5
    internalexception_retries: 5
  jetstream2:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: jetstream2
  stampede3:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: stampede3
  frontera:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: frontera
  bridges:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: bridges
  expanse:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: expanse
  rockfish:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: rockfish
  rockfish_devgalaxy:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    manager: rockfish_devgalaxy

galaxy_job_conf_default_container_id: /cvmfs/singularity.galaxyproject.org/all/python:3.8.3

galaxy_job_conf_singularity_volumes:
  local:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "{{ galaxy_object_store_cache_path }}:ro"
    # what was this needed for?
    #- "{{ galaxy_new_file_path }}:ro"
    - "/corral4/{{ galaxy_instance_codename }}/objects:ro"
    - "/corral4/{{ galaxy_instance_codename }}/files:ro"
    - "/corral4/{{ galaxy_instance_codename }}/psufiles:ro"
    - "/corral4/data:ro"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  jetstream:
    - "$galaxy_root:ro"
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/cvmfs/{{ galaxy_instance_codename }}.galaxyproject.org:ro"
  bridges:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "/local:rw"
  expanse:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "$CVMFSEXEC_DIR/.cvmfsexec/dist/cvmfs/data.galaxyproject.org:/cvmfs/data.galaxyproject.org:ro"
    - "/scratch:rw"
  rockfish:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/tmp:rw"
  rockfish_devgalaxy:
    - "$tool_directory:ro"
    - "$working_directory:rw"
    - "$job_directory:rw"
    - "/tmp:rw"
  # the tacc-apptainer module mostly automatically sets up mounts, but we need to ensure that /tmp inside the container
  # is always the host /tmp (e.g. in trinity containers it is /var/tmp)
  tacc_hpc:
    - "/cvmfs/data.galaxyproject.org:ro"
    - "/tmp:/tmp:rw"
    - "/tmp:/var/tmp:rw"

# TODO: this is the contents of wraptar, needs to be deployed somehow:
#
# #!/bin/bash
#
# wraptar_version='0.1.0'
#
# echo ''
# echo "Running ceph wraptar $wraptar_version around tar to suppress changed file warning" >&2
# echo ''
#
# if ! { stderr="$( { TAR_OPTIONS='--warning=no-file-changed' /usr/local/bin/tar "$@"; } 2>&1 1>&3 3>&- )"; } 3>&1; then
#     rc=$?
#     if [ -n "$stderr" ]; then
#         echo "$stderr" >&2
#         exit $rc
#     fi
# fi

# explicit mappings in the "tools" section of the job_conf
galaxy_job_conf_tools:
  - id: interactive_tool_jupyter_notebook
    handler: k8s
  - id: interactive_tool_rstudio
    handler: k8s
  - id: interactive_tool_panoply
    handler: k8s
  - id: interactive_tool_phinch
    handler: k8s
  - id: interactive_tool_pavian
    handler: k8s
  - id: interactive_tool_qiskit_jupyter_notebook
    handler: k8s
  - id: interactive_tool_blobtoolkit
    handler: k8s

galaxy_job_conf_limits:
  # this is a failsafe more than anything - actual limits are enforced on the environments
  - type: registered_user_concurrent_jobs
    value: 8
  - type: anonymous_user_concurrent_jobs
    value: 1

  # these probably don't really work
  - type: walltime
    value: '194:00:00'
  - type: output_size
    value: 200G

  # pulsar envs
  - type: environment_user_concurrent_jobs
    id: jetstream2_k8s
    value: 1

  # tpv per-environment user limits
  - type: environment_user_concurrent_jobs
    id: roundup_small
    value: 3
  - type: environment_user_concurrent_jobs
    id: roundup
    value: 3
  - type: environment_user_concurrent_jobs
    id: roundup_mem
    value: 1
  - type: environment_user_concurrent_jobs
    id: jetstream2
    value: 4
  - type: environment_user_concurrent_jobs
    id: bridges2
    value: 1
  - type: environment_user_concurrent_jobs
    id: expanse
    value: 1
  - type: environment_user_concurrent_jobs
    id: rockfish
    value: 1
  - type: environment_user_concurrent_jobs
    id: stampede3_skx
    value: 1
  - type: environment_user_concurrent_jobs
    id: stampede3_icx
    value: 1

  # tpv per-environment total limits
  - type: environment_total_concurrent_jobs
    id: bridges2
    value: 100
  - type: environment_total_concurrent_jobs
    id: expanse
    # technically 64 for compute, 4096 for shared, but this prevents us from burning all of expanse at once
    value: 64
  - type: environment_total_concurrent_jobs
    id: rockfish
    # doesn't have a published limit
    value: 50
  - type: environment_total_concurrent_jobs
    id: frontera
    value: 20
  - type: environment_total_concurrent_jobs
    id: stampede3_skx
    value: 20
  - type: environment_total_concurrent_jobs
    id: stampede3_icx
    value: 20

#pulsar_default_file_action: remote_transfer_tus
pulsar_default_file_action: remote_transfer

## used by: galaxyproject.galaxy
galaxy_errordocs_dest: "{{ nginx_srv }}/{{ galaxy_instance_hostname }}/error"
galaxy_errordocs_502_message: |
  You are seeing this message because a request to Galaxy timed out or was refused. This may be a temporary issue which
  could be resolved by retrying the operation you were performing. If you receive this message repeatedly or for an
  extended amount of time, please check for additional information on the
  <a href="https://status.galaxyproject.org/">Galaxy status page</a> or the
  <a href="https://mstdn.science/@galaxyproject">@galaxyproject Mastodon feed</a>. If the issue is not addressed on those
  sources, you may report it to the support team at
  <a href='mailto:galaxy-bugs@galaxyproject.org'>galaxy-bugs@galaxyproject.org</a>
  with details on what you were trying to do and the URL in the address bar.


## used by usegalaxy_privileged and templating job_conf.yml
galaxy_dynamic_rule_dir: "{{ galaxy_root }}/dynamic_rules"

galaxy_pgcleanup_actions:
  - delete_userless_histories
  - delete_exported_histories
  #- purge_deleted_users
  # this is less for GDPR purposes and more for allowing users to reregister with the same account later
  - purge_deleted_users_gdpr
  - purge_deleted_histories
  - purge_deleted_hdas
  - purge_historyless_hdas
  - purge_error_hdas
  - purge_hdas_of_purged_histories
  - delete_datasets
  - purge_datasets
galaxy_pgcleanup_days: 30
galaxy_pgcleanup_old_hdas:
  - object_store_id: corral4-scratch
    days: 30

## used by: usegalaxy_admin
galaxy_log_archive_dir: /corral4/{{ galaxy_instance_codename }}/backup/log

# We don't use galaxy_local_tools because we want to set a section
galaxy_local_tools_dir: "/corral4/gxsrc/{{ galaxy_instance_codename }}/local_tools"
galaxy_local_tools:
  # For reasons I wasn't able to get to the bottom of, when served from RStudio directly, the large static javascript
  # files sent by rserver stop sending prematurely and never complete on the client side, only when sent over tailscale,
  # but are fine when proxied with nginx.
  - file: interactivetool_rstudio.xml
    section_name: Interactive Tools

# galaxy_config hash moved to group_vars/all/galaxy_config_vars.yml

tpv_mutable_dir: "{{ galaxy_root }}/tpv_staging"
tpv_config_dir_name: "tpv"
# the role defauls set this, but we need it before the role is imported
tpv_config_dir: "{{ galaxy_config_dir }}/{{ tpv_config_dir_name }}"
tpv_configs:
  - templates/galaxy/config/tpv/defaults.yaml.j2
  - templates/galaxy/config/tpv/environments.yaml.j2
  - templates/galaxy/config/tpv/tools_pre_shared.yaml.j2
  - templates/galaxy/config/tpv/tools.yaml.j2
  - templates/galaxy/config/tpv/users.yaml.j2
  - templates/galaxy/config/tpv/roles.yaml.j2

# specifies config files to copy from the playbook
galaxy_config_files:
  - src: files/galaxy/config/panel_views/
    dest: "{{ galaxy_config.galaxy.panel_views_dir }}"
  - src: files/galaxy/config/tool_data_table_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: files/galaxy/config/tool_conf.xml
    dest: "{{ galaxy_config_dir }}/tool_conf.xml"
  - src: files/galaxy/config/data_manager_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['data_manager_config_file'] }}"
  - src: files/galaxy/config/tool_sheds_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['tool_sheds_config_file'] }}"
  - src: files/galaxy/config/job_metrics_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_metrics_config_file'] }}"
  - src: files/galaxy/config/job_resource_params_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['job_resource_params_file'] }}"
  - src: files/galaxy/config/dependency_resolvers_conf.xml
    dest: "{{ galaxy_config[galaxy_app_config_section]['dependency_resolvers_config_file'] }}"
  - src: files/galaxy/config/disposable_email_blacklist.conf
    dest: "{{ galaxy_config[galaxy_app_config_section]['email_domain_blocklist_file'] }}"
  - src: files/galaxy/config/workflow_schedulers_conf.xml
    dest: "{{ galaxy_config_dir }}/workflow_schedulers_conf.xml"
  - src: files/galaxy/config/user_preferences.yml
    dest: "{{ galaxy_config[galaxy_app_config_section]['user_preferences_extra_conf_path'] }}"
  - src: files/galaxy/config/trs_servers_conf.yml
    dest: "{{ galaxy_config_dir }}/trs_servers_conf.yml"

# specifies config files to template from the playbook
# NOTE: changes here may need to be reflected in galaxy-vgp host_vars
galaxy_config_templates:
  - src: templates/galaxy/config/job_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: templates/galaxy/config/build_sites.yml.j2
    dest: "{{ galaxy_config[galaxy_app_config_section]['build_sites_config_file'] }}"
  - src: templates/galaxy/config/tacc_k8s_pulsar_app_config.yml.j2
    dest: "{{ galaxy_config_dir }}/tacc_k8s_pulsar_app_config.yml"
  - src: templates/galaxy/config/file_sources_conf.yml.j2
    dest: "{{ galaxy_config_dir }}/file_sources_conf.yml"
